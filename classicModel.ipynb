{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ[\"export CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'age', 'ISS', 'ecog', 'bonelesion', 'BUN', 'Totalprotein', 'Glucose', 'Hb', 'PLT', 'ALB', 'LDH', 'Cr', 'Ca', 'B2MG', 'ANC', 'WBC', 'Group']\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_excel('train.xlsx')\n",
    "test = pd.read_excel('test.xlsx')\n",
    "\n",
    "target = ['Group']\n",
    "cat_feats = ['sex', 'ISS', 'ecog', 'bonelesion'] #'sex', 'ISS', 'ecog', 'bonelesion'\n",
    "dense_feats = ['age', 'BUN', 'Totalprotein', 'Glucose', 'Hb', 'PLT', 'ALB', 'LDH', 'Cr', 'Ca', 'B2MG', 'ANC', 'WBC']\n",
    "\n",
    "print(train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "def process(df, cat_feats, dense_feats, is_process_null=True, is_normalize=True, inplace=False, dense_transformer=None, target=\"Group\", le=None):\n",
    "    \"\"\"\n",
    "    数据处理。\n",
    "    1. 填补缺失值;\n",
    "    2. 将类别变量展开为 one-hot;\n",
    "    3. 数值特征的处理;\n",
    "    4. 类别不平衡处理;\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object and col != target:\n",
    "            df[col][~df[col].apply(lambda x: type(x)).isin([float, int])] = np.nan\n",
    "    \n",
    "    if is_process_null:\n",
    "        # 填补缺失值\n",
    "        for col in df.columns:\n",
    "            num_null = df[col].isnull().sum()\n",
    "            if num_null == 0:\n",
    "                continue\n",
    "            if col in cat_feats:\n",
    "                value = df[col].value_counts().sort_values(ascending=False).index[0]\n",
    "            else:\n",
    "                value = df[col].quantile(q=0.5)\n",
    "            df[col].fillna(value=value, inplace=True)\n",
    "    \n",
    "    # 处理类别特征\n",
    "    for cf in cat_feats:\n",
    "        tmp = pd.get_dummies(df[cf])\n",
    "        tmp.columns = [f\"{cf}_{i}\" for i in range(tmp.shape[1])]\n",
    "        df = pd.concat([df, tmp], axis=1)\n",
    "        \n",
    "    df.drop(cat_feats, inplace=True, axis=1)\n",
    "    \n",
    "    if is_normalize:\n",
    "        # 处理数值特征\n",
    "#         transformers = {}\n",
    "#         for denf in dense_feats:\n",
    "#             scaler = preprocessing.StandardScaler().fit(df[denf].values.reshape((-1, 1)))\n",
    "#             transformers[denf] = scaler\n",
    "#             df[denf] = scaler.transform(df[denf].values)\n",
    "        df = df.astype(dict(zip(dense_feats, [float]*len(dense_feats))))\n",
    "        if not dense_transformer:\n",
    "            scaler = preprocessing.StandardScaler().fit(df[dense_feats].values)\n",
    "        else:\n",
    "            scaler = dense_transformer\n",
    "        df[dense_feats] = scaler.transform(df[dense_feats])\n",
    "        transformer = scaler\n",
    "    else:\n",
    "        transformer = None\n",
    "        \n",
    "    # 对目标变量进行编码\n",
    "    if le is None:\n",
    "        le = preprocessing.LabelEncoder().fit(df[target])\n",
    "    df[target] = le.transform(df[target])\n",
    "    \n",
    "    # 处理类别不平衡\n",
    "\n",
    "    \n",
    "    return df, transformer, le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一种数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 240, 0: 240})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE,SMOTEN\n",
    "from collections import Counter\n",
    "\n",
    "ptra, trans, le = process(train, cat_feats, dense_feats)\n",
    "ptes, *_ = process(test, cat_feats, dense_feats, dense_transformer=trans, le=le)\n",
    "\n",
    "y = ptra['Group'].values\n",
    "ptra.drop(target, axis=1, inplace=True)\n",
    "X = ptra.values\n",
    "x_train, y_train = X, y\n",
    "\n",
    "y_test = ptes[target].values\n",
    "ptes.drop(target, axis=1, inplace=True)\n",
    "x_test = ptes.values\n",
    "\n",
    "# 降维\n",
    "# pca = PCA(n_components=5)\n",
    "# X_vis=pca.fit_transform(x_train)\n",
    "\n",
    "#以使用imblearn进行随机过采样\n",
    "# sm=RandomUnderSampler(random_state=0)\n",
    "sm = SMOTE()\n",
    "X_resampled, y_resampled=sm.fit_resample(x_train, y_train)\n",
    "# X_res_vis=pca.transform(X_resampled)\n",
    "x_train, y_train = X_resampled, y_resampled\n",
    "Counter(y_resampled)\n",
    "# x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二种数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE,SMOTEN\n",
    "\n",
    "file_train = pd.read_excel(\"train.xlsx\").fillna(value=0)\n",
    "file_test = pd.read_excel(\"test.xlsx\").fillna(value=0)\n",
    "file_train[\"Glucose\"] = file_train[\"Glucose\"].apply(lambda x: 0 if type(x) == str else float(x))\n",
    "file_train[\"BUN\"] = file_train[\"BUN\"].apply(lambda x: 0 if type(x) == str else float(x))\n",
    "file_test[\"Glucose\"] = file_test[\"Glucose\"].apply(lambda x: 0 if type(x) == str else float(x))\n",
    "file_test[\"BUN\"] = file_test[\"BUN\"].apply(lambda x: 0 if type(x) == str else float(x))\n",
    "file_test[\"Group\"] = file_test[\"Group\"].apply(lambda x: 1 if x == \"FHR\" else 0)\n",
    "file_train[\"Group\"] = file_train[\"Group\"].apply(lambda x: 1 if x == \"FHR\" else 0)\n",
    "\n",
    "\n",
    "x_train = file_train.iloc[:, :-1]\n",
    "y_train = file_train[\"Group\"].astype(np.int64)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = file_test.iloc[:, :-1]\n",
    "y_test = file_test[\"Group\"].astype(np.int64)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "std_x_train = StandardScaler().fit_transform(x_train)\n",
    "std_x_test = StandardScaler().fit_transform(x_test)\n",
    "\n",
    "#以使用imblearn进行随机过采样为例\n",
    "# sm = SMOTE()\n",
    "# X_resampled, y_resampled=sm.fit_resample(x_train, y_train)\n",
    "# x_train, y_train = X_resampled, y_resampled\n",
    "\n",
    "# #查看结果\n",
    "# Counter(y_resampled) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost 寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.490 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.438 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.646 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.542 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.500 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.531 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.729 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.719 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.729 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.490 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.583 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.635 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.812 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.771 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.698 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.625 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.802 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.906 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.812 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.833 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.844 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.802 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.708 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.635 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.792 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.781 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.625 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.688 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.698 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.625 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.802 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.885 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.854 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.812 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.875 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.875 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.823 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.896 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.865 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.854 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.625 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.812 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.875 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.865 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.812 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.656 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.844 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.760 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.562 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.792 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.896 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.844 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.688 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.823 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.938 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.885 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.823 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.688 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.865 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.938 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.906 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.917 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.6857894736842105, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.6857894736842105, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.6857894736842105, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.6857894736842105, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.6857894736842105, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.688 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.781 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.917 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.844 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.781 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.531 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.573 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.781 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.875 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.854 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.865 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.531 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.771 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.771 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.812 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.760 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.802 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.812 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.688 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.750 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.781 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.875 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.844 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.802 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.438 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.792 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.854 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.719 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.531 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.729 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.573 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.854 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.594 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.812 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.844 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.865 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.760 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.875 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.812 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.573 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.781 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.781 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.917 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.792 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.750 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.938 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.854 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.729 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.812 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.812 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.740 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.573 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.781 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.875 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.823 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.802 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.583 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.854 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.865 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.885 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.875 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.854 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.812 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.646 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.771 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.885 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.833 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.833 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.823 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.896 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.854 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.573 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.844 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.833 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.583 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.635 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.844 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.677 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.792 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.802 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.635 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.854 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.802 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.7431578947368421, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.677 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.844 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.927 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.948 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.11473684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.875 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.833 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.854 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.844 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.781 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.688 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.896 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.781 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.812 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.583 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.906 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.802 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.740 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.604 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.844 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.740 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.740 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.01, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.656 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.844 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.958 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.896 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.917 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.573 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.927 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.844 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.885 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.865 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.781 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.708 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.562 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.875 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.688 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.531 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.677 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.802 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.760 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.4763157894736842, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.615 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.792 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.854 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.740 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.573 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.760 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=1, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.771 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.615 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.854 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.802 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.854 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.266842105263158, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.677 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.844 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.833 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.917 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.844 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.823 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.6857894736842105, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.448 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.6857894736842105, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.521 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.6857894736842105, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.729 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.6857894736842105, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.6857894736842105, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.656 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.781 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.802 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.11473684210526315, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.656 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.781 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.719 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.656 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.625 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.729 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.740 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.531 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.646 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.802 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.812 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.792 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.844 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.833 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.698 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.688 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.802 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.792 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.635 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.906 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.875 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.781 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.562 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.812 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.729 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.740 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.573 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.510 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.500 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.771 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.802 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.854 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.844 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.1621052631578948, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.802 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.677 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.781 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.781 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.750 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.646 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.823 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.896 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.875 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.781 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.927 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.865 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.9526315789473684, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.510 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.719 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.708 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.708 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.573 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.729 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.802 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=2, classifier__max_depth=8, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.615 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.812 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.812 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.510 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.635 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.9526315789473684, classifier__gamma=5, classifier__max_depth=9, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.562 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.573 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.688 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=1, classifier__max_depth=8, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.635 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.781 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.844 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.583 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.708 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.760 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.698 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=10, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.792 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=2.0, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.677 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.844 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=0.8;, score=0.844 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.562 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.906 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.802 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=0.4;, score=0.812 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.7431578947368421, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.11473684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.531 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.531 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.500 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.625 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=2.0, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.500 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.688 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.906 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.875 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.896 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.802 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.740 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.6;, score=0.781 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.7905263157894737, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.635 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.802 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.906 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.906 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.885 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.719 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.781 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.917 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.844 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.844 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.656 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.812 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.906 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.875 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.854 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.719 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.719 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.688 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.01, classifier__gamma=5, classifier__max_depth=4, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.521 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.562 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.698 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.781 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.812 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.7431578947368421, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.792 total time=   0.2s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.646 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.781 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.906 total time=   0.2s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.823 total time=   0.2s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.865 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.792 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=5, classifier__max_depth=3, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.490 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.781 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.781 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=2, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.646 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.646 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.833 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.875 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.885 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.823 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.656 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.750 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.802 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.792 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=8, classifier__min_child_weight=0.8, classifier__subsample=0.4;, score=0.698 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.1621052631578948, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.708 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.719 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.438 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.625 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.740 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.771 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=7, classifier__min_child_weight=10, classifier__subsample=0.4;, score=0.677 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=2, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=0.5, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.802 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.917 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=2.0, classifier__gamma=1, classifier__max_depth=6, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.771 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.688 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.760 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.823 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.875 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.802 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.9526315789473684, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.604 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.708 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.0573684210526315, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.6;, score=0.854 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.5810526315789473, classifier__gamma=0.5, classifier__max_depth=5, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.635 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.854 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.750 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.638421052631579, classifier__gamma=5, classifier__max_depth=5, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.875 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.635 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.802 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.927 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.854 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.5336842105263158, classifier__gamma=1, classifier__max_depth=9, classifier__min_child_weight=0.8, classifier__subsample=0.8;, score=0.865 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.42894736842105263, classifier__gamma=2, classifier__max_depth=3, classifier__min_child_weight=0.8, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.646 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.781 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.854 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.771 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.656 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.812 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.865 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.906 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=1.5, classifier__max_depth=7, classifier__min_child_weight=0.5, classifier__subsample=1.0;, score=0.760 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.552 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.688 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.781 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.719 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.771 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=9, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.646 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.833 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.958 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.906 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.3242105263157895, classifier__gamma=1, classifier__max_depth=5, classifier__min_child_weight=0.8, classifier__subsample=0.6;, score=0.865 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.594 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.812 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=0.4;, score=0.708 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.677 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.812 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.823 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.4763157894736842, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.521 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.469 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.531 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.740 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.8952631578947368, classifier__gamma=1.5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.760 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.823 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.844 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=10, classifier__subsample=0.6;, score=0.781 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.510 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.812 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.865 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.812 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=5, classifier__subsample=1.0;, score=0.792 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.638421052631579, classifier__gamma=1.5, classifier__max_depth=8, classifier__min_child_weight=1, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.5336842105263158, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.479 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.5336842105263158, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.5336842105263158, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.792 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.5336842105263158, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.854 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.5336842105263158, classifier__gamma=0.5, classifier__max_depth=1, classifier__min_child_weight=5, classifier__subsample=0.8;, score=0.750 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.823 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.844 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=1, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=0.8;, score=0.802 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.21947368421052632, classifier__gamma=1.5, classifier__max_depth=3, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.677 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.896 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.833 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.6857894736842105, classifier__gamma=0.5, classifier__max_depth=4, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.833 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=0.8478947368421053, classifier__gamma=0.5, classifier__max_depth=2, classifier__min_child_weight=10, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.604 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.792 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.917 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.896 total time=   0.1s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.8478947368421053, classifier__gamma=2, classifier__max_depth=9, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.833 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.698 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.740 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.7905263157894737, classifier__gamma=0.5, classifier__max_depth=3, classifier__min_child_weight=10, classifier__subsample=1.0;, score=0.812 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.5810526315789473, classifier__gamma=5, classifier__max_depth=1, classifier__min_child_weight=1, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.562 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.771 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.875 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.833 total time=   0.2s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.9526315789473684, classifier__gamma=2, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.6;, score=0.771 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.625 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.760 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.854 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.875 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.4, classifier__eta=1.8952631578947368, classifier__gamma=0.5, classifier__max_depth=7, classifier__min_child_weight=0.8, classifier__subsample=1.0;, score=0.854 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.6, classifier__eta=1.266842105263158, classifier__gamma=2, classifier__max_depth=5, classifier__min_child_weight=0.5, classifier__subsample=1.4;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.656 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.802 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.885 total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.833 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=0.3242105263157895, classifier__gamma=1.5, classifier__max_depth=4, classifier__min_child_weight=0.5, classifier__subsample=0.4;, score=0.812 total time=   0.1s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.562 total time=   0.1s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.729 total time=   0.1s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.823 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.854 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=1.371578947368421, classifier__gamma=0.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=0.8;, score=0.781 total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=1.0, classifier__eta=0.8478947368421053, classifier__gamma=1.5, classifier__max_depth=6, classifier__min_child_weight=0.5, classifier__subsample=1.2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.646 total time=   0.0s\n",
      "[CV 2/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.740 total time=   0.0s\n",
      "[CV 3/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.885 total time=   0.1s\n",
      "[CV 4/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.729 total time=   0.0s\n",
      "[CV 5/5] END classifier__colsample_bytree=0.8, classifier__eta=1.4763157894736842, classifier__gamma=5, classifier__max_depth=2, classifier__min_child_weight=1, classifier__subsample=1.0;, score=0.833 total time=   0.0s\n",
      "{'classifier__subsample': 0.8, 'classifier__min_child_weight': 1, 'classifier__max_depth': 5, 'classifier__gamma': 0.5, 'classifier__eta': 0.3242105263157895, 'classifier__colsample_bytree': 1.0}\n",
      "0.7167630057803468\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[('classifier', XGBClassifier(eval_metric=['auc']))])\n",
    "params = {\n",
    "\n",
    "    'classifier__min_child_weight': [0.5, 0.8, 1, 5, 10],\n",
    "    'classifier__eta': np.linspace(0.01,2,20),\n",
    "    # 'classifier__base_score': np.linspace(0.01,0.99,20),\n",
    "    #\n",
    "    'classifier__gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    #\n",
    "    'classifier__subsample': [0.4, 0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "    #\n",
    "    'classifier__colsample_bytree': [0.4, 0.6, 0.8, 1.0],\n",
    "    #\n",
    "    'classifier__max_depth': range(1,10)\n",
    "\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb_pipeline, param_distributions=params, n_iter=200,\n",
    "                                   verbose=3, random_state=1001)\n",
    "random_search.fit(x_train, y_train)\n",
    "print(random_search.best_params_)\n",
    "print(random_search.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.25626204238921\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.25626204238921\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.25626204238921\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.25626204238921\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.25626204238921\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.25626204238921\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.74373795761079\n",
      "0.5\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7321772639691715\n",
      "1.0\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6936416184971098\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:30:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6917148362235067\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6859344894026975\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7360308285163777\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7341040462427746\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7321772639691715\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7321772639691715\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:30:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6917148362235067\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6917148362235067\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7341040462427746\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6936416184971098\n",
      "1.0\n",
      "[19:30:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6917148362235067\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:30:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7321772639691715\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6936416184971098\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7321772639691715\n",
      "1.0\n",
      "[19:30:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:30:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:30:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:10] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:11] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7360308285163777\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:13] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7379576107899807\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7244701348747592\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6917148362235067\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6936416184971098\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7013487475915221\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7263969171483622\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.697495183044316\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6917148362235067\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:37] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7052023121387283\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6936416184971098\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7321772639691715\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6994219653179191\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7032755298651252\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7148362235067437\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7071290944123314\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7167630057803468\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7225433526011561\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7129094412331407\n",
      "1.0\n",
      "[19:31:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7109826589595376\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7186897880539499\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.720616570327553\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7090558766859345\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7360308285163777\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7302504816955684\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7283236994219653\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "[19:31:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.6955684007707129\n",
      "1.0\n",
      "0.0180980980980981 0.6081460127001441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "result =  1\n",
    "best_auc = 0\n",
    "for i in np.linspace(0.001,0.9,1000):\n",
    "\n",
    "    xgb_clf = XGBClassifier(subsample=0.8, min_child_weight=1, classifier__max_depth=5, classifier__gamma=0.5,\n",
    "                        classifier__colsample_bytree=1.0,eta=0.32,base_score=i)\n",
    "    xgb_clf.fit(x_train, y_train)\n",
    "    xgb_y_pred = xgb_clf.predict(x_test)\n",
    "    y_pred_proba = xgb_clf.predict_proba(x_test)\n",
    "    print(xgb_clf.score(x_test, y_test))\n",
    "    print(xgb_clf.score(x_train, y_train))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "    xgb_auc_score = roc_auc_score(y_test, xgb_clf.predict_proba(x_test)[:, 1])\n",
    "\n",
    "    if xgb_auc_score >best_auc:\n",
    "        best_auc = xgb_auc_score\n",
    "        result = i\n",
    "\n",
    "print(result,best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier 寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, min_samples_leaf=16, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, min_samples_leaf=9, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, min_samples_leaf=12, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, min_samples_leaf=12, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, min_samples_leaf=12, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, min_samples_leaf=12, splitter=best;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, min_samples_leaf=12, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.600 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, min_samples_leaf=3, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, min_samples_leaf=3, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, min_samples_leaf=3, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, min_samples_leaf=3, splitter=random;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, min_samples_leaf=3, splitter=random;, score=0.593 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, min_samples_leaf=9, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, min_samples_leaf=14, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, min_samples_leaf=14, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, min_samples_leaf=14, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, min_samples_leaf=14, splitter=best;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, min_samples_leaf=14, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, min_samples_leaf=3, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, min_samples_leaf=3, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, min_samples_leaf=3, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, min_samples_leaf=3, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, min_samples_leaf=3, splitter=best;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, min_samples_leaf=11, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, min_samples_leaf=11, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, min_samples_leaf=11, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, min_samples_leaf=13, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, min_samples_leaf=13, splitter=best;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, min_samples_leaf=13, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.600 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, min_samples_leaf=13, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, min_samples_leaf=13, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, min_samples_leaf=13, splitter=best;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, min_samples_leaf=13, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, min_samples_leaf=10, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, min_samples_leaf=10, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, min_samples_leaf=10, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, min_samples_leaf=10, splitter=random;, score=0.695 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, min_samples_leaf=2, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, min_samples_leaf=2, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, min_samples_leaf=2, splitter=random;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, min_samples_leaf=13, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, min_samples_leaf=13, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, min_samples_leaf=13, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, min_samples_leaf=13, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, min_samples_leaf=13, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, min_samples_leaf=18, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, min_samples_leaf=18, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, min_samples_leaf=18, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, min_samples_leaf=18, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=16, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=15, min_samples_leaf=19, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=15, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=15, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=15, min_samples_leaf=19, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=15, min_samples_leaf=19, splitter=best;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, min_samples_leaf=4, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, min_samples_leaf=4, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, min_samples_leaf=4, splitter=random;, score=0.633 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, min_samples_leaf=4, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, min_samples_leaf=4, splitter=random;, score=0.746 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, min_samples_leaf=1, splitter=random;, score=0.650 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, min_samples_leaf=1, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, min_samples_leaf=1, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, min_samples_leaf=1, splitter=random;, score=0.650 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, min_samples_leaf=1, splitter=random;, score=0.542 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, min_samples_leaf=9, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, min_samples_leaf=9, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, min_samples_leaf=9, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, min_samples_leaf=9, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, min_samples_leaf=9, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, min_samples_leaf=7, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, min_samples_leaf=7, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, min_samples_leaf=7, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, min_samples_leaf=7, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, min_samples_leaf=7, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=random;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=random;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=13, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=13, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=13, splitter=best;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=13, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, min_samples_leaf=16, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, min_samples_leaf=16, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, min_samples_leaf=16, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, min_samples_leaf=16, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, min_samples_leaf=16, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=18, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=18, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=18, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, min_samples_leaf=4, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, min_samples_leaf=4, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, min_samples_leaf=4, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, min_samples_leaf=4, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, min_samples_leaf=4, splitter=best;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, min_samples_leaf=1, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=7, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=7, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=7, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=7, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=7, splitter=best;, score=0.746 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, min_samples_leaf=18, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=12, splitter=best;, score=0.583 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=12, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=12, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=12, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=12, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, min_samples_leaf=7, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, min_samples_leaf=7, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, min_samples_leaf=7, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, min_samples_leaf=7, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, min_samples_leaf=7, splitter=best;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, min_samples_leaf=7, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, min_samples_leaf=7, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, min_samples_leaf=7, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, min_samples_leaf=7, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, min_samples_leaf=7, splitter=random;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, min_samples_leaf=3, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, min_samples_leaf=3, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, min_samples_leaf=3, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, min_samples_leaf=3, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, min_samples_leaf=3, splitter=best;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.617 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.746 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=best;, score=0.817 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=best;, score=0.695 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, min_samples_leaf=10, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, min_samples_leaf=10, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, min_samples_leaf=10, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, min_samples_leaf=10, splitter=best;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, min_samples_leaf=10, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, min_samples_leaf=11, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, min_samples_leaf=7, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, min_samples_leaf=7, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, min_samples_leaf=7, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, min_samples_leaf=7, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, min_samples_leaf=7, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, min_samples_leaf=10, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, min_samples_leaf=10, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, min_samples_leaf=10, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, min_samples_leaf=10, splitter=best;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, min_samples_leaf=10, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=19, splitter=random;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, min_samples_leaf=2, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, min_samples_leaf=2, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, min_samples_leaf=2, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, min_samples_leaf=2, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, min_samples_leaf=2, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, min_samples_leaf=13, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, min_samples_leaf=13, splitter=best;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, min_samples_leaf=13, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, min_samples_leaf=1, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, min_samples_leaf=1, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, min_samples_leaf=1, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, min_samples_leaf=1, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, min_samples_leaf=1, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, min_samples_leaf=8, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, min_samples_leaf=8, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, min_samples_leaf=8, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, min_samples_leaf=8, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, min_samples_leaf=8, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, min_samples_leaf=9, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, min_samples_leaf=9, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, min_samples_leaf=9, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, min_samples_leaf=9, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, min_samples_leaf=9, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, min_samples_leaf=9, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, min_samples_leaf=9, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, min_samples_leaf=9, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, min_samples_leaf=9, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, min_samples_leaf=12, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, min_samples_leaf=16, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, min_samples_leaf=17, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, min_samples_leaf=17, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, min_samples_leaf=17, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, min_samples_leaf=10, splitter=best;, score=0.600 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, min_samples_leaf=10, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, min_samples_leaf=10, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, min_samples_leaf=10, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, min_samples_leaf=10, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, min_samples_leaf=3, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, min_samples_leaf=3, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, min_samples_leaf=3, splitter=best;, score=0.617 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, min_samples_leaf=3, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, min_samples_leaf=3, splitter=best;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, min_samples_leaf=9, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, min_samples_leaf=9, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, min_samples_leaf=9, splitter=random;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, min_samples_leaf=4, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, min_samples_leaf=4, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, min_samples_leaf=4, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, min_samples_leaf=4, splitter=best;, score=0.617 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, min_samples_leaf=4, splitter=best;, score=0.695 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, min_samples_leaf=11, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, min_samples_leaf=11, splitter=random;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, min_samples_leaf=18, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, min_samples_leaf=18, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, min_samples_leaf=18, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, min_samples_leaf=1, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, min_samples_leaf=1, splitter=best;, score=0.617 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, min_samples_leaf=1, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, min_samples_leaf=1, splitter=best;, score=0.567 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, min_samples_leaf=1, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, min_samples_leaf=3, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, min_samples_leaf=3, splitter=random;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, min_samples_leaf=3, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, min_samples_leaf=3, splitter=random;, score=0.650 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, min_samples_leaf=3, splitter=random;, score=0.695 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, min_samples_leaf=8, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, min_samples_leaf=8, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, min_samples_leaf=8, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, min_samples_leaf=8, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, min_samples_leaf=8, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, min_samples_leaf=6, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, min_samples_leaf=6, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, min_samples_leaf=6, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, min_samples_leaf=6, splitter=best;, score=0.600 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, min_samples_leaf=6, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=10, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=10, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=10, splitter=random;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, min_samples_leaf=16, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, min_samples_leaf=16, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, min_samples_leaf=16, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, min_samples_leaf=16, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, min_samples_leaf=16, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, min_samples_leaf=16, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, min_samples_leaf=16, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, min_samples_leaf=16, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, min_samples_leaf=16, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, min_samples_leaf=16, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=15, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=15, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=15, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=15, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=15, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=15, min_samples_leaf=6, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=15, min_samples_leaf=6, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=15, min_samples_leaf=6, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=15, min_samples_leaf=6, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=15, min_samples_leaf=6, splitter=random;, score=0.695 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=best;, score=0.817 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.600 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, min_samples_leaf=13, splitter=random;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=2, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=2, splitter=random;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=2, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=2, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=2, splitter=random;, score=0.610 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, min_samples_leaf=14, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, min_samples_leaf=14, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, min_samples_leaf=11, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, min_samples_leaf=11, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, min_samples_leaf=11, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, min_samples_leaf=4, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, min_samples_leaf=4, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, min_samples_leaf=4, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, min_samples_leaf=4, splitter=best;, score=0.817 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, min_samples_leaf=4, splitter=best;, score=0.746 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, min_samples_leaf=8, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, min_samples_leaf=8, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, min_samples_leaf=8, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, min_samples_leaf=8, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, min_samples_leaf=8, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, min_samples_leaf=17, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, min_samples_leaf=19, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=14, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=14, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=14, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=14, splitter=best;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=14, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, min_samples_leaf=9, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, min_samples_leaf=9, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, min_samples_leaf=9, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, min_samples_leaf=9, splitter=best;, score=0.600 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, min_samples_leaf=9, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, min_samples_leaf=7, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, min_samples_leaf=7, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, min_samples_leaf=7, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, min_samples_leaf=7, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, min_samples_leaf=7, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, min_samples_leaf=15, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, min_samples_leaf=17, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=17, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, min_samples_leaf=5, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, min_samples_leaf=8, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, min_samples_leaf=8, splitter=best;, score=0.617 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, min_samples_leaf=8, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, min_samples_leaf=8, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, min_samples_leaf=8, splitter=best;, score=0.831 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, min_samples_leaf=15, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, min_samples_leaf=14, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, min_samples_leaf=1, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, min_samples_leaf=1, splitter=best;, score=0.617 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, min_samples_leaf=1, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, min_samples_leaf=1, splitter=best;, score=0.567 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, min_samples_leaf=1, splitter=best;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.617 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, min_samples_leaf=10, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, min_samples_leaf=16, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, min_samples_leaf=10, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, min_samples_leaf=10, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, min_samples_leaf=10, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, min_samples_leaf=10, splitter=best;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, min_samples_leaf=10, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, min_samples_leaf=10, splitter=best;, score=0.600 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, min_samples_leaf=10, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, min_samples_leaf=10, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, min_samples_leaf=10, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, min_samples_leaf=10, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, min_samples_leaf=1, splitter=best;, score=0.567 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, min_samples_leaf=1, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, min_samples_leaf=1, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, min_samples_leaf=1, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, min_samples_leaf=1, splitter=best;, score=0.746 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=2, splitter=best;, score=0.833 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=2, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=2, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=2, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=2, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, min_samples_leaf=13, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, min_samples_leaf=13, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, min_samples_leaf=14, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, min_samples_leaf=14, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, min_samples_leaf=14, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, min_samples_leaf=14, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, min_samples_leaf=14, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, min_samples_leaf=12, splitter=best;, score=0.583 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, min_samples_leaf=12, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, min_samples_leaf=12, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, min_samples_leaf=12, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, min_samples_leaf=12, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, min_samples_leaf=2, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, min_samples_leaf=2, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, min_samples_leaf=2, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, min_samples_leaf=17, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=15, min_samples_leaf=12, splitter=best;, score=0.583 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=15, min_samples_leaf=12, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=15, min_samples_leaf=12, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=15, min_samples_leaf=12, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=15, min_samples_leaf=12, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, min_samples_leaf=12, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, min_samples_leaf=12, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, min_samples_leaf=12, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, min_samples_leaf=12, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, min_samples_leaf=12, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, min_samples_leaf=12, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, min_samples_leaf=13, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, min_samples_leaf=13, splitter=best;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, min_samples_leaf=13, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, min_samples_leaf=4, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, min_samples_leaf=4, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, min_samples_leaf=4, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, min_samples_leaf=4, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, min_samples_leaf=4, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, min_samples_leaf=15, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, min_samples_leaf=18, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, min_samples_leaf=6, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, min_samples_leaf=6, splitter=random;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, min_samples_leaf=6, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, min_samples_leaf=6, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, min_samples_leaf=6, splitter=random;, score=0.746 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=3, splitter=random;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, min_samples_leaf=11, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, min_samples_leaf=10, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, min_samples_leaf=10, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, min_samples_leaf=10, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, min_samples_leaf=10, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, min_samples_leaf=10, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=random;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, min_samples_leaf=2, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, min_samples_leaf=2, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, min_samples_leaf=2, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, min_samples_leaf=2, splitter=best;, score=0.617 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, min_samples_leaf=2, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=8, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=8, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=8, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=8, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=8, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=best;, score=0.600 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=best;, score=0.678 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, min_samples_leaf=5, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, min_samples_leaf=5, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, min_samples_leaf=5, splitter=random;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, min_samples_leaf=5, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, min_samples_leaf=5, splitter=random;, score=0.695 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, min_samples_leaf=4, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, min_samples_leaf=4, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, min_samples_leaf=4, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, min_samples_leaf=4, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, min_samples_leaf=4, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, min_samples_leaf=15, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, min_samples_leaf=15, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, min_samples_leaf=14, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, min_samples_leaf=17, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, min_samples_leaf=17, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, min_samples_leaf=17, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, min_samples_leaf=17, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, min_samples_leaf=17, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, min_samples_leaf=15, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, min_samples_leaf=10, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, min_samples_leaf=10, splitter=random;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, min_samples_leaf=2, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, min_samples_leaf=2, splitter=random;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, min_samples_leaf=2, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, min_samples_leaf=2, splitter=random;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, min_samples_leaf=2, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=13, min_samples_leaf=18, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=13, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=13, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=13, min_samples_leaf=18, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=13, min_samples_leaf=18, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, min_samples_leaf=18, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, min_samples_leaf=1, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, min_samples_leaf=1, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, min_samples_leaf=1, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, min_samples_leaf=1, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, min_samples_leaf=1, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, min_samples_leaf=3, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, min_samples_leaf=3, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, min_samples_leaf=3, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, min_samples_leaf=3, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, min_samples_leaf=3, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, min_samples_leaf=13, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, min_samples_leaf=13, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, min_samples_leaf=10, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, min_samples_leaf=2, splitter=random;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, min_samples_leaf=2, splitter=random;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, min_samples_leaf=2, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, min_samples_leaf=2, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, min_samples_leaf=2, splitter=random;, score=0.576 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, min_samples_leaf=10, splitter=random;, score=0.833 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, min_samples_leaf=10, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, min_samples_leaf=10, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, min_samples_leaf=10, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=13, min_samples_leaf=15, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, min_samples_leaf=16, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, min_samples_leaf=4, splitter=random;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, min_samples_leaf=13, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, min_samples_leaf=13, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, min_samples_leaf=13, splitter=best;, score=0.500 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, min_samples_leaf=13, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, min_samples_leaf=4, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, min_samples_leaf=4, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, min_samples_leaf=4, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, min_samples_leaf=4, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, min_samples_leaf=4, splitter=best;, score=0.678 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, min_samples_leaf=3, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, min_samples_leaf=3, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, min_samples_leaf=3, splitter=random;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, min_samples_leaf=3, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, min_samples_leaf=3, splitter=random;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=best;, score=0.817 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, min_samples_leaf=2, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, min_samples_leaf=5, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, min_samples_leaf=5, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, min_samples_leaf=5, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, min_samples_leaf=5, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, min_samples_leaf=5, splitter=best;, score=0.661 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, min_samples_leaf=10, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, min_samples_leaf=10, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, min_samples_leaf=19, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, min_samples_leaf=2, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, min_samples_leaf=2, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, min_samples_leaf=2, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, min_samples_leaf=2, splitter=best;, score=0.817 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, min_samples_leaf=2, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, min_samples_leaf=7, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, min_samples_leaf=7, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, min_samples_leaf=7, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, min_samples_leaf=7, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, min_samples_leaf=7, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, min_samples_leaf=4, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, min_samples_leaf=4, splitter=random;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, min_samples_leaf=4, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, min_samples_leaf=4, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, min_samples_leaf=4, splitter=random;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, min_samples_leaf=18, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, min_samples_leaf=18, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, min_samples_leaf=18, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, min_samples_leaf=6, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, min_samples_leaf=6, splitter=best;, score=0.650 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, min_samples_leaf=6, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, min_samples_leaf=6, splitter=best;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, min_samples_leaf=6, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, min_samples_leaf=2, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=5, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=5, splitter=random;, score=0.644 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, min_samples_leaf=6, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, min_samples_leaf=6, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, min_samples_leaf=6, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, min_samples_leaf=6, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, min_samples_leaf=6, splitter=best;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=6, splitter=random;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=6, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=6, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=6, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=6, splitter=random;, score=0.729 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=15, min_samples_leaf=18, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=15, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=15, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=15, min_samples_leaf=18, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=15, min_samples_leaf=18, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, min_samples_leaf=14, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, min_samples_leaf=14, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, min_samples_leaf=14, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, min_samples_leaf=14, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, min_samples_leaf=14, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=13, min_samples_leaf=5, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=13, min_samples_leaf=5, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=13, min_samples_leaf=5, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=13, min_samples_leaf=5, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=13, min_samples_leaf=5, splitter=random;, score=0.831 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, min_samples_leaf=17, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, min_samples_leaf=17, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, min_samples_leaf=8, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, min_samples_leaf=8, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, min_samples_leaf=8, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, min_samples_leaf=8, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, min_samples_leaf=8, splitter=best;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, min_samples_leaf=13, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, min_samples_leaf=13, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, min_samples_leaf=13, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, min_samples_leaf=13, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, min_samples_leaf=13, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, min_samples_leaf=13, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, min_samples_leaf=5, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, min_samples_leaf=19, splitter=best;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, min_samples_leaf=18, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, min_samples_leaf=18, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=11, min_samples_leaf=2, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=11, min_samples_leaf=2, splitter=random;, score=0.700 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=11, min_samples_leaf=2, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=11, min_samples_leaf=2, splitter=random;, score=0.633 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=11, min_samples_leaf=2, splitter=random;, score=0.746 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, min_samples_leaf=16, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, min_samples_leaf=16, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, min_samples_leaf=18, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, min_samples_leaf=18, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, min_samples_leaf=18, splitter=best;, score=0.667 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, min_samples_leaf=18, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, min_samples_leaf=6, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, min_samples_leaf=6, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, min_samples_leaf=6, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, min_samples_leaf=6, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, min_samples_leaf=6, splitter=random;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=best;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, min_samples_leaf=15, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, min_samples_leaf=9, splitter=random;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, min_samples_leaf=15, splitter=best;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, min_samples_leaf=15, splitter=best;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, min_samples_leaf=15, splitter=best;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, min_samples_leaf=15, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, min_samples_leaf=15, splitter=best;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, min_samples_leaf=11, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, min_samples_leaf=11, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, min_samples_leaf=11, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, min_samples_leaf=11, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.833 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.683 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.650 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, min_samples_leaf=7, splitter=random;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, min_samples_leaf=8, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, min_samples_leaf=8, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, min_samples_leaf=8, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, min_samples_leaf=8, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, min_samples_leaf=8, splitter=random;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, min_samples_leaf=13, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, min_samples_leaf=13, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.733 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, min_samples_leaf=1, splitter=best;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, min_samples_leaf=1, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, min_samples_leaf=1, splitter=random;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, min_samples_leaf=1, splitter=random;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, min_samples_leaf=1, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, min_samples_leaf=1, splitter=random;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, min_samples_leaf=19, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, min_samples_leaf=15, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, min_samples_leaf=15, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, min_samples_leaf=14, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, min_samples_leaf=8, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, min_samples_leaf=8, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, min_samples_leaf=8, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, min_samples_leaf=8, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, min_samples_leaf=8, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, min_samples_leaf=2, splitter=random;, score=0.717 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, min_samples_leaf=2, splitter=random;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, min_samples_leaf=2, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, min_samples_leaf=2, splitter=random;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, min_samples_leaf=2, splitter=random;, score=0.712 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, min_samples_leaf=6, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, min_samples_leaf=6, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, min_samples_leaf=6, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, min_samples_leaf=6, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, min_samples_leaf=6, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, min_samples_leaf=8, splitter=random;, score=0.817 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, min_samples_leaf=8, splitter=random;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, min_samples_leaf=8, splitter=random;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, min_samples_leaf=8, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, min_samples_leaf=8, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, min_samples_leaf=19, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, min_samples_leaf=14, splitter=random;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, min_samples_leaf=14, splitter=random;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, min_samples_leaf=1, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, min_samples_leaf=1, splitter=best;, score=0.633 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, min_samples_leaf=1, splitter=best;, score=0.700 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, min_samples_leaf=1, splitter=best;, score=0.567 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, min_samples_leaf=1, splitter=best;, score=0.763 total time=   0.0s\n",
      "{'splitter': 'random', 'min_samples_leaf': 16, 'max_depth': 16, 'criterion': 'entropy'}\n",
      "0.74373795761079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# DecisionTreeClassifier().get_params().keys()\n",
    "dt = DecisionTreeClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best','random'],\n",
    "    'min_samples_leaf': range(1,20),\n",
    "    'max_depth': range(1,20)\n",
    "}\n",
    "random_search = RandomizedSearchCV(dt, param_distributions=params, n_iter=200,\n",
    "                                   verbose=3, random_state=1001)\n",
    "random_search.fit(x_train, y_train)\n",
    "print(random_search.best_params_)\n",
    "print(random_search.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# DecisionTreeClassifier().get_params().keys()\n",
    "dt = DecisionTreeClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best','random'],\n",
    "    'min_samples_leaf': range(1,20),\n",
    "    'max_depth': range(1,20)\n",
    "}\n",
    "random_search = RandomizedSearchCV(dt, param_distributions=params, n_iter=200,\n",
    "                                   verbose=3, random_state=1001)\n",
    "random_search.fit(x_train, y_train)\n",
    "print(random_search.best_params_)\n",
    "print(random_search.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "result =  1\n",
    "best_auc = 0\n",
    "for i in np.linspace(0.001,0.9,1000):\n",
    "\n",
    "    xgb_clf = XGBClassifier(subsample=0.8, min_child_weight=1, classifier__max_depth=5, classifier__gamma=0.5,\n",
    "                        classifier__colsample_bytree=1.0,eta=0.32,base_score=i)\n",
    "    xgb_clf.fit(x_train, y_train)\n",
    "    xgb_y_pred = xgb_clf.predict(x_test)\n",
    "    y_pred_proba = xgb_clf.predict_proba(x_test)\n",
    "    print(xgb_clf.score(x_test, y_test))\n",
    "    print(xgb_clf.score(x_train, y_train))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "    xgb_auc_score = roc_auc_score(y_test, xgb_clf.predict_proba(x_test)[:, 1])\n",
    "\n",
    "    if xgb_auc_score >best_auc:\n",
    "        best_auc = xgb_auc_score\n",
    "        result = i\n",
    "\n",
    "print(result,best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier 寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[CV 1/5] END criterion=gini, max_depth=19, max_features=10, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=14, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, max_features=10, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=14, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, max_features=10, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=14, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, max_features=10, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=14, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, max_features=10, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=14, n_estimators=130;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=19, max_leaf_nodes=47, min_samples_leaf=26, min_samples_split=15, n_estimators=171;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=19, max_leaf_nodes=47, min_samples_leaf=26, min_samples_split=15, n_estimators=171;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=19, max_leaf_nodes=47, min_samples_leaf=26, min_samples_split=15, n_estimators=171;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=19, max_leaf_nodes=47, min_samples_leaf=26, min_samples_split=15, n_estimators=171;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=19, max_leaf_nodes=47, min_samples_leaf=26, min_samples_split=15, n_estimators=171;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, max_features=24, max_leaf_nodes=38, min_samples_leaf=14, min_samples_split=2, n_estimators=158;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, max_features=24, max_leaf_nodes=38, min_samples_leaf=14, min_samples_split=2, n_estimators=158;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, max_features=24, max_leaf_nodes=38, min_samples_leaf=14, min_samples_split=2, n_estimators=158;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, max_features=24, max_leaf_nodes=38, min_samples_leaf=14, min_samples_split=2, n_estimators=158;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, max_features=24, max_leaf_nodes=38, min_samples_leaf=14, min_samples_split=2, n_estimators=158;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, max_features=25, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=6, n_estimators=195;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, max_features=25, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=6, n_estimators=195;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, max_features=25, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=6, n_estimators=195;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, max_features=25, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=6, n_estimators=195;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, max_features=25, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=6, n_estimators=195;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=45, min_samples_leaf=21, min_samples_split=17, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=45, min_samples_leaf=21, min_samples_split=17, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=45, min_samples_leaf=21, min_samples_split=17, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=45, min_samples_leaf=21, min_samples_split=17, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=45, min_samples_leaf=21, min_samples_split=17, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=18, max_leaf_nodes=47, min_samples_leaf=15, min_samples_split=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=18, max_leaf_nodes=47, min_samples_leaf=15, min_samples_split=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=18, max_leaf_nodes=47, min_samples_leaf=15, min_samples_split=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=18, max_leaf_nodes=47, min_samples_leaf=15, min_samples_split=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=18, max_leaf_nodes=47, min_samples_leaf=15, min_samples_split=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=20, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=19, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=20, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=19, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=20, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=19, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=20, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=19, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=20, max_leaf_nodes=45, min_samples_leaf=12, min_samples_split=19, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=41;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=41;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=41;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=41;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=41;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=14, max_leaf_nodes=35, min_samples_leaf=3, min_samples_split=16, n_estimators=5;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=14, max_leaf_nodes=35, min_samples_leaf=3, min_samples_split=16, n_estimators=5;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=14, max_leaf_nodes=35, min_samples_leaf=3, min_samples_split=16, n_estimators=5;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=14, max_leaf_nodes=35, min_samples_leaf=3, min_samples_split=16, n_estimators=5;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=14, max_leaf_nodes=35, min_samples_leaf=3, min_samples_split=16, n_estimators=5;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=39, min_samples_leaf=4, min_samples_split=13, n_estimators=59;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=39, min_samples_leaf=4, min_samples_split=13, n_estimators=59;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=39, min_samples_leaf=4, min_samples_split=13, n_estimators=59;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=39, min_samples_leaf=4, min_samples_split=13, n_estimators=59;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=39, min_samples_leaf=4, min_samples_split=13, n_estimators=59;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=35, min_samples_leaf=11, min_samples_split=14, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=35, min_samples_leaf=11, min_samples_split=14, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=35, min_samples_leaf=11, min_samples_split=14, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=35, min_samples_leaf=11, min_samples_split=14, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, max_features=21, max_leaf_nodes=35, min_samples_leaf=11, min_samples_split=14, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=28, min_samples_leaf=14, min_samples_split=10, n_estimators=101;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=28, min_samples_leaf=14, min_samples_split=10, n_estimators=101;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=28, min_samples_leaf=14, min_samples_split=10, n_estimators=101;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=28, min_samples_leaf=14, min_samples_split=10, n_estimators=101;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=28, min_samples_leaf=14, min_samples_split=10, n_estimators=101;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=15, max_features=19, max_leaf_nodes=29, min_samples_leaf=3, min_samples_split=11, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=15, max_features=19, max_leaf_nodes=29, min_samples_leaf=3, min_samples_split=11, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=15, max_features=19, max_leaf_nodes=29, min_samples_leaf=3, min_samples_split=11, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=15, max_features=19, max_leaf_nodes=29, min_samples_leaf=3, min_samples_split=11, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=15, max_features=19, max_leaf_nodes=29, min_samples_leaf=3, min_samples_split=11, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=12, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=18, n_estimators=142;, score=0.783 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=12, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=18, n_estimators=142;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=12, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=18, n_estimators=142;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=12, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=18, n_estimators=142;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=12, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=18, n_estimators=142;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=11, n_estimators=63;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=11, n_estimators=63;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=11, n_estimators=63;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=11, n_estimators=63;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=11, n_estimators=63;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=30, min_samples_leaf=5, min_samples_split=11, n_estimators=8;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=30, min_samples_leaf=5, min_samples_split=11, n_estimators=8;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=30, min_samples_leaf=5, min_samples_split=11, n_estimators=8;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=30, min_samples_leaf=5, min_samples_split=11, n_estimators=8;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=30, min_samples_leaf=5, min_samples_split=11, n_estimators=8;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=38, min_samples_leaf=28, min_samples_split=15, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=38, min_samples_leaf=28, min_samples_split=15, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=38, min_samples_leaf=28, min_samples_split=15, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=38, min_samples_leaf=28, min_samples_split=15, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=38, min_samples_leaf=28, min_samples_split=15, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, max_features=18, max_leaf_nodes=49, min_samples_leaf=23, min_samples_split=2, n_estimators=172;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, max_features=18, max_leaf_nodes=49, min_samples_leaf=23, min_samples_split=2, n_estimators=172;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, max_features=18, max_leaf_nodes=49, min_samples_leaf=23, min_samples_split=2, n_estimators=172;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, max_features=18, max_leaf_nodes=49, min_samples_leaf=23, min_samples_split=2, n_estimators=172;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, max_features=18, max_leaf_nodes=49, min_samples_leaf=23, min_samples_split=2, n_estimators=172;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, max_features=11, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=4, n_estimators=143;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, max_features=11, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=4, n_estimators=143;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, max_features=11, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=4, n_estimators=143;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, max_features=11, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=4, n_estimators=143;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, max_features=11, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=4, n_estimators=143;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=30, min_samples_leaf=16, min_samples_split=12, n_estimators=21;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=30, min_samples_leaf=16, min_samples_split=12, n_estimators=21;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=30, min_samples_leaf=16, min_samples_split=12, n_estimators=21;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=30, min_samples_leaf=16, min_samples_split=12, n_estimators=21;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=30, min_samples_leaf=16, min_samples_split=12, n_estimators=21;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=13, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=13, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=13, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=13, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=13, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, max_features=6, max_leaf_nodes=40, min_samples_leaf=23, min_samples_split=8, n_estimators=191;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, max_features=6, max_leaf_nodes=40, min_samples_leaf=23, min_samples_split=8, n_estimators=191;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, max_features=6, max_leaf_nodes=40, min_samples_leaf=23, min_samples_split=8, n_estimators=191;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, max_features=6, max_leaf_nodes=40, min_samples_leaf=23, min_samples_split=8, n_estimators=191;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, max_features=6, max_leaf_nodes=40, min_samples_leaf=23, min_samples_split=8, n_estimators=191;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, max_features=19, max_leaf_nodes=31, min_samples_leaf=25, min_samples_split=7, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, max_features=19, max_leaf_nodes=31, min_samples_leaf=25, min_samples_split=7, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, max_features=19, max_leaf_nodes=31, min_samples_leaf=25, min_samples_split=7, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, max_features=19, max_leaf_nodes=31, min_samples_leaf=25, min_samples_split=7, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, max_features=19, max_leaf_nodes=31, min_samples_leaf=25, min_samples_split=7, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, max_features=17, max_leaf_nodes=34, min_samples_leaf=25, min_samples_split=20, n_estimators=129;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, max_features=17, max_leaf_nodes=34, min_samples_leaf=25, min_samples_split=20, n_estimators=129;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, max_features=17, max_leaf_nodes=34, min_samples_leaf=25, min_samples_split=20, n_estimators=129;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, max_features=17, max_leaf_nodes=34, min_samples_leaf=25, min_samples_split=20, n_estimators=129;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, max_features=17, max_leaf_nodes=34, min_samples_leaf=25, min_samples_split=20, n_estimators=129;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, max_features=16, max_leaf_nodes=46, min_samples_leaf=3, min_samples_split=14, n_estimators=58;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, max_features=16, max_leaf_nodes=46, min_samples_leaf=3, min_samples_split=14, n_estimators=58;, score=0.767 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, max_features=16, max_leaf_nodes=46, min_samples_leaf=3, min_samples_split=14, n_estimators=58;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, max_features=16, max_leaf_nodes=46, min_samples_leaf=3, min_samples_split=14, n_estimators=58;, score=0.817 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, max_features=16, max_leaf_nodes=46, min_samples_leaf=3, min_samples_split=14, n_estimators=58;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, max_features=29, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=11, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, max_features=29, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=11, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, max_features=29, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=11, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, max_features=29, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=11, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, max_features=29, max_leaf_nodes=44, min_samples_leaf=6, min_samples_split=11, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=36, min_samples_leaf=27, min_samples_split=11, n_estimators=126;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=36, min_samples_leaf=27, min_samples_split=11, n_estimators=126;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=36, min_samples_leaf=27, min_samples_split=11, n_estimators=126;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=36, min_samples_leaf=27, min_samples_split=11, n_estimators=126;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=36, min_samples_leaf=27, min_samples_split=11, n_estimators=126;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, max_features=20, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=12, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, max_features=20, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=12, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, max_features=20, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=12, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, max_features=20, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=12, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, max_features=20, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=12, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=20, max_leaf_nodes=27, min_samples_leaf=19, min_samples_split=9, n_estimators=78;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=20, max_leaf_nodes=27, min_samples_leaf=19, min_samples_split=9, n_estimators=78;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=20, max_leaf_nodes=27, min_samples_leaf=19, min_samples_split=9, n_estimators=78;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=20, max_leaf_nodes=27, min_samples_leaf=19, min_samples_split=9, n_estimators=78;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=20, max_leaf_nodes=27, min_samples_leaf=19, min_samples_split=9, n_estimators=78;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=10, max_leaf_nodes=30, min_samples_leaf=29, min_samples_split=16, n_estimators=60;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=10, max_leaf_nodes=30, min_samples_leaf=29, min_samples_split=16, n_estimators=60;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=10, max_leaf_nodes=30, min_samples_leaf=29, min_samples_split=16, n_estimators=60;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=10, max_leaf_nodes=30, min_samples_leaf=29, min_samples_split=16, n_estimators=60;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=10, max_leaf_nodes=30, min_samples_leaf=29, min_samples_split=16, n_estimators=60;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, max_features=24, max_leaf_nodes=25, min_samples_leaf=23, min_samples_split=20, n_estimators=173;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, max_features=24, max_leaf_nodes=25, min_samples_leaf=23, min_samples_split=20, n_estimators=173;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, max_features=24, max_leaf_nodes=25, min_samples_leaf=23, min_samples_split=20, n_estimators=173;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, max_features=24, max_leaf_nodes=25, min_samples_leaf=23, min_samples_split=20, n_estimators=173;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, max_features=24, max_leaf_nodes=25, min_samples_leaf=23, min_samples_split=20, n_estimators=173;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, max_features=22, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=5, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, max_features=22, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=5, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, max_features=22, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=5, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, max_features=22, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=5, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, max_features=22, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=5, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=26, min_samples_leaf=7, min_samples_split=17, n_estimators=143;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=26, min_samples_leaf=7, min_samples_split=17, n_estimators=143;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=26, min_samples_leaf=7, min_samples_split=17, n_estimators=143;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=26, min_samples_leaf=7, min_samples_split=17, n_estimators=143;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=5, max_leaf_nodes=26, min_samples_leaf=7, min_samples_split=17, n_estimators=143;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=20, max_leaf_nodes=49, min_samples_leaf=13, min_samples_split=11, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=20, max_leaf_nodes=49, min_samples_leaf=13, min_samples_split=11, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=20, max_leaf_nodes=49, min_samples_leaf=13, min_samples_split=11, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=20, max_leaf_nodes=49, min_samples_leaf=13, min_samples_split=11, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=20, max_leaf_nodes=49, min_samples_leaf=13, min_samples_split=11, n_estimators=84;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, max_features=6, max_leaf_nodes=46, min_samples_leaf=10, min_samples_split=17, n_estimators=27;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, max_features=6, max_leaf_nodes=46, min_samples_leaf=10, min_samples_split=17, n_estimators=27;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, max_features=6, max_leaf_nodes=46, min_samples_leaf=10, min_samples_split=17, n_estimators=27;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, max_features=6, max_leaf_nodes=46, min_samples_leaf=10, min_samples_split=17, n_estimators=27;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, max_features=6, max_leaf_nodes=46, min_samples_leaf=10, min_samples_split=17, n_estimators=27;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=13, max_features=28, max_leaf_nodes=39, min_samples_leaf=3, min_samples_split=7, n_estimators=90;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=13, max_features=28, max_leaf_nodes=39, min_samples_leaf=3, min_samples_split=7, n_estimators=90;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=13, max_features=28, max_leaf_nodes=39, min_samples_leaf=3, min_samples_split=7, n_estimators=90;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=13, max_features=28, max_leaf_nodes=39, min_samples_leaf=3, min_samples_split=7, n_estimators=90;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=13, max_features=28, max_leaf_nodes=39, min_samples_leaf=3, min_samples_split=7, n_estimators=90;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, max_features=15, max_leaf_nodes=47, min_samples_leaf=20, min_samples_split=7, n_estimators=177;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, max_features=15, max_leaf_nodes=47, min_samples_leaf=20, min_samples_split=7, n_estimators=177;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, max_features=15, max_leaf_nodes=47, min_samples_leaf=20, min_samples_split=7, n_estimators=177;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, max_features=15, max_leaf_nodes=47, min_samples_leaf=20, min_samples_split=7, n_estimators=177;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, max_features=15, max_leaf_nodes=47, min_samples_leaf=20, min_samples_split=7, n_estimators=177;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, max_features=21, max_leaf_nodes=27, min_samples_leaf=15, min_samples_split=10, n_estimators=87;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, max_features=21, max_leaf_nodes=27, min_samples_leaf=15, min_samples_split=10, n_estimators=87;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, max_features=21, max_leaf_nodes=27, min_samples_leaf=15, min_samples_split=10, n_estimators=87;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, max_features=21, max_leaf_nodes=27, min_samples_leaf=15, min_samples_split=10, n_estimators=87;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, max_features=21, max_leaf_nodes=27, min_samples_leaf=15, min_samples_split=10, n_estimators=87;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=11, max_features=16, max_leaf_nodes=29, min_samples_leaf=17, min_samples_split=7, n_estimators=52;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=11, max_features=16, max_leaf_nodes=29, min_samples_leaf=17, min_samples_split=7, n_estimators=52;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=11, max_features=16, max_leaf_nodes=29, min_samples_leaf=17, min_samples_split=7, n_estimators=52;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=11, max_features=16, max_leaf_nodes=29, min_samples_leaf=17, min_samples_split=7, n_estimators=52;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=11, max_features=16, max_leaf_nodes=29, min_samples_leaf=17, min_samples_split=7, n_estimators=52;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=38, min_samples_leaf=26, min_samples_split=9, n_estimators=137;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=38, min_samples_leaf=26, min_samples_split=9, n_estimators=137;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=38, min_samples_leaf=26, min_samples_split=9, n_estimators=137;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=38, min_samples_leaf=26, min_samples_split=9, n_estimators=137;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, max_features=19, max_leaf_nodes=38, min_samples_leaf=26, min_samples_split=9, n_estimators=137;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=15, max_features=21, max_leaf_nodes=36, min_samples_leaf=2, min_samples_split=11, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=15, max_features=21, max_leaf_nodes=36, min_samples_leaf=2, min_samples_split=11, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=15, max_features=21, max_leaf_nodes=36, min_samples_leaf=2, min_samples_split=11, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=15, max_features=21, max_leaf_nodes=36, min_samples_leaf=2, min_samples_split=11, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=15, max_features=21, max_leaf_nodes=36, min_samples_leaf=2, min_samples_split=11, n_estimators=12;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, max_features=29, max_leaf_nodes=33, min_samples_leaf=4, min_samples_split=6, n_estimators=165;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, max_features=29, max_leaf_nodes=33, min_samples_leaf=4, min_samples_split=6, n_estimators=165;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, max_features=29, max_leaf_nodes=33, min_samples_leaf=4, min_samples_split=6, n_estimators=165;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, max_features=29, max_leaf_nodes=33, min_samples_leaf=4, min_samples_split=6, n_estimators=165;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, max_features=29, max_leaf_nodes=33, min_samples_leaf=4, min_samples_split=6, n_estimators=165;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=33, min_samples_leaf=5, min_samples_split=7, n_estimators=199;, score=0.783 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=33, min_samples_leaf=5, min_samples_split=7, n_estimators=199;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=33, min_samples_leaf=5, min_samples_split=7, n_estimators=199;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=33, min_samples_leaf=5, min_samples_split=7, n_estimators=199;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=33, min_samples_leaf=5, min_samples_split=7, n_estimators=199;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=9, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=9, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=9, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=9, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=9, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, max_features=26, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=19, n_estimators=47;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, max_features=26, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=19, n_estimators=47;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, max_features=26, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=19, n_estimators=47;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, max_features=26, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=19, n_estimators=47;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, max_features=26, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=19, n_estimators=47;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, max_features=14, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=5, n_estimators=36;, score=0.783 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, max_features=14, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=5, n_estimators=36;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, max_features=14, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=5, n_estimators=36;, score=0.817 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, max_features=14, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=5, n_estimators=36;, score=0.783 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, max_features=14, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=5, n_estimators=36;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=21, max_leaf_nodes=37, min_samples_leaf=11, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=21, max_leaf_nodes=37, min_samples_leaf=11, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=21, max_leaf_nodes=37, min_samples_leaf=11, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=21, max_leaf_nodes=37, min_samples_leaf=11, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=21, max_leaf_nodes=37, min_samples_leaf=11, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, max_features=21, max_leaf_nodes=29, min_samples_leaf=29, min_samples_split=5, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, max_features=21, max_leaf_nodes=29, min_samples_leaf=29, min_samples_split=5, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, max_features=21, max_leaf_nodes=29, min_samples_leaf=29, min_samples_split=5, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, max_features=21, max_leaf_nodes=29, min_samples_leaf=29, min_samples_split=5, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, max_features=21, max_leaf_nodes=29, min_samples_leaf=29, min_samples_split=5, n_estimators=82;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=13, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=12, n_estimators=160;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=13, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=12, n_estimators=160;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=13, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=12, n_estimators=160;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=13, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=12, n_estimators=160;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=13, max_leaf_nodes=46, min_samples_leaf=16, min_samples_split=12, n_estimators=160;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=21, max_leaf_nodes=29, min_samples_leaf=2, min_samples_split=5, n_estimators=176;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=21, max_leaf_nodes=29, min_samples_leaf=2, min_samples_split=5, n_estimators=176;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=21, max_leaf_nodes=29, min_samples_leaf=2, min_samples_split=5, n_estimators=176;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=21, max_leaf_nodes=29, min_samples_leaf=2, min_samples_split=5, n_estimators=176;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=21, max_leaf_nodes=29, min_samples_leaf=2, min_samples_split=5, n_estimators=176;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, max_features=14, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=3, n_estimators=168;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, max_features=14, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=3, n_estimators=168;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, max_features=14, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=3, n_estimators=168;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, max_features=14, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=3, n_estimators=168;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, max_features=14, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=3, n_estimators=168;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=16, max_leaf_nodes=43, min_samples_leaf=15, min_samples_split=20, n_estimators=63;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=16, max_leaf_nodes=43, min_samples_leaf=15, min_samples_split=20, n_estimators=63;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=16, max_leaf_nodes=43, min_samples_leaf=15, min_samples_split=20, n_estimators=63;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=16, max_leaf_nodes=43, min_samples_leaf=15, min_samples_split=20, n_estimators=63;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=16, max_leaf_nodes=43, min_samples_leaf=15, min_samples_split=20, n_estimators=63;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=12, max_leaf_nodes=35, min_samples_leaf=7, min_samples_split=6, n_estimators=32;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=12, max_leaf_nodes=35, min_samples_leaf=7, min_samples_split=6, n_estimators=32;, score=0.783 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=12, max_leaf_nodes=35, min_samples_leaf=7, min_samples_split=6, n_estimators=32;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=12, max_leaf_nodes=35, min_samples_leaf=7, min_samples_split=6, n_estimators=32;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=12, max_leaf_nodes=35, min_samples_leaf=7, min_samples_split=6, n_estimators=32;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, max_features=6, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=18, n_estimators=172;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, max_features=6, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=18, n_estimators=172;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, max_features=6, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=18, n_estimators=172;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, max_features=6, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=18, n_estimators=172;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, max_features=6, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=18, n_estimators=172;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=9, n_estimators=92;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=9, n_estimators=92;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=9, n_estimators=92;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=9, n_estimators=92;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=36, min_samples_leaf=11, min_samples_split=9, n_estimators=92;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, max_features=23, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=13, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, max_features=23, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=13, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, max_features=23, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=13, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, max_features=23, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=13, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, max_features=23, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=13, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=35, min_samples_leaf=16, min_samples_split=5, n_estimators=9;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=35, min_samples_leaf=16, min_samples_split=5, n_estimators=9;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=35, min_samples_leaf=16, min_samples_split=5, n_estimators=9;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=35, min_samples_leaf=16, min_samples_split=5, n_estimators=9;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=26, max_leaf_nodes=35, min_samples_leaf=16, min_samples_split=5, n_estimators=9;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=6, n_estimators=169;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=6, n_estimators=169;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=6, n_estimators=169;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=6, n_estimators=169;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, max_features=26, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=6, n_estimators=169;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=14, n_estimators=43;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=14, n_estimators=43;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=14, n_estimators=43;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=14, n_estimators=43;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=14, n_estimators=43;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=37, min_samples_leaf=21, min_samples_split=11, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=37, min_samples_leaf=21, min_samples_split=11, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=37, min_samples_leaf=21, min_samples_split=11, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=37, min_samples_leaf=21, min_samples_split=11, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=37, min_samples_leaf=21, min_samples_split=11, n_estimators=178;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, max_features=25, max_leaf_nodes=29, min_samples_leaf=15, min_samples_split=11, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, max_features=25, max_leaf_nodes=29, min_samples_leaf=15, min_samples_split=11, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, max_features=25, max_leaf_nodes=29, min_samples_leaf=15, min_samples_split=11, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, max_features=25, max_leaf_nodes=29, min_samples_leaf=15, min_samples_split=11, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, max_features=25, max_leaf_nodes=29, min_samples_leaf=15, min_samples_split=11, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, max_features=28, max_leaf_nodes=47, min_samples_leaf=10, min_samples_split=2, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, max_features=28, max_leaf_nodes=47, min_samples_leaf=10, min_samples_split=2, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, max_features=28, max_leaf_nodes=47, min_samples_leaf=10, min_samples_split=2, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, max_features=28, max_leaf_nodes=47, min_samples_leaf=10, min_samples_split=2, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, max_features=28, max_leaf_nodes=47, min_samples_leaf=10, min_samples_split=2, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=21, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=7, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=21, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=7, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=21, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=7, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=21, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=7, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=21, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=7, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=15, max_features=20, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=8, n_estimators=41;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=15, max_features=20, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=8, n_estimators=41;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=15, max_features=20, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=8, n_estimators=41;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=15, max_features=20, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=8, n_estimators=41;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=15, max_features=20, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=8, n_estimators=41;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=25, max_leaf_nodes=34, min_samples_leaf=15, min_samples_split=15, n_estimators=13;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=25, max_leaf_nodes=34, min_samples_leaf=15, min_samples_split=15, n_estimators=13;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=25, max_leaf_nodes=34, min_samples_leaf=15, min_samples_split=15, n_estimators=13;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=25, max_leaf_nodes=34, min_samples_leaf=15, min_samples_split=15, n_estimators=13;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=25, max_leaf_nodes=34, min_samples_leaf=15, min_samples_split=15, n_estimators=13;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=28, min_samples_leaf=11, min_samples_split=15, n_estimators=136;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=28, min_samples_leaf=11, min_samples_split=15, n_estimators=136;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=28, min_samples_leaf=11, min_samples_split=15, n_estimators=136;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=28, min_samples_leaf=11, min_samples_split=15, n_estimators=136;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, max_features=23, max_leaf_nodes=28, min_samples_leaf=11, min_samples_split=15, n_estimators=136;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=23, max_leaf_nodes=40, min_samples_leaf=22, min_samples_split=4, n_estimators=30;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=23, max_leaf_nodes=40, min_samples_leaf=22, min_samples_split=4, n_estimators=30;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=23, max_leaf_nodes=40, min_samples_leaf=22, min_samples_split=4, n_estimators=30;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=23, max_leaf_nodes=40, min_samples_leaf=22, min_samples_split=4, n_estimators=30;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=23, max_leaf_nodes=40, min_samples_leaf=22, min_samples_split=4, n_estimators=30;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=29, max_leaf_nodes=29, min_samples_leaf=9, min_samples_split=18, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=29, max_leaf_nodes=29, min_samples_leaf=9, min_samples_split=18, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=29, max_leaf_nodes=29, min_samples_leaf=9, min_samples_split=18, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=29, max_leaf_nodes=29, min_samples_leaf=9, min_samples_split=18, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=29, max_leaf_nodes=29, min_samples_leaf=9, min_samples_split=18, n_estimators=174;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, max_features=21, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=21, n_estimators=121;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, max_features=21, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=21, n_estimators=121;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, max_features=21, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=21, n_estimators=121;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, max_features=21, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=21, n_estimators=121;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, max_features=21, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=21, n_estimators=121;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=37, min_samples_leaf=19, min_samples_split=6, n_estimators=106;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=37, min_samples_leaf=19, min_samples_split=6, n_estimators=106;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=37, min_samples_leaf=19, min_samples_split=6, n_estimators=106;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=37, min_samples_leaf=19, min_samples_split=6, n_estimators=106;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=37, min_samples_leaf=19, min_samples_split=6, n_estimators=106;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, max_features=18, max_leaf_nodes=48, min_samples_leaf=13, min_samples_split=4, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, max_features=18, max_leaf_nodes=48, min_samples_leaf=13, min_samples_split=4, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, max_features=18, max_leaf_nodes=48, min_samples_leaf=13, min_samples_split=4, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, max_features=18, max_leaf_nodes=48, min_samples_leaf=13, min_samples_split=4, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, max_features=18, max_leaf_nodes=48, min_samples_leaf=13, min_samples_split=4, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=6, max_leaf_nodes=49, min_samples_leaf=24, min_samples_split=10, n_estimators=17;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=6, max_leaf_nodes=49, min_samples_leaf=24, min_samples_split=10, n_estimators=17;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=6, max_leaf_nodes=49, min_samples_leaf=24, min_samples_split=10, n_estimators=17;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=6, max_leaf_nodes=49, min_samples_leaf=24, min_samples_split=10, n_estimators=17;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=6, max_leaf_nodes=49, min_samples_leaf=24, min_samples_split=10, n_estimators=17;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, max_features=13, max_leaf_nodes=30, min_samples_leaf=19, min_samples_split=2, n_estimators=116;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, max_features=13, max_leaf_nodes=30, min_samples_leaf=19, min_samples_split=2, n_estimators=116;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, max_features=13, max_leaf_nodes=30, min_samples_leaf=19, min_samples_split=2, n_estimators=116;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, max_features=13, max_leaf_nodes=30, min_samples_leaf=19, min_samples_split=2, n_estimators=116;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, max_features=13, max_leaf_nodes=30, min_samples_leaf=19, min_samples_split=2, n_estimators=116;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=15, max_features=26, max_leaf_nodes=46, min_samples_leaf=2, min_samples_split=2, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=15, max_features=26, max_leaf_nodes=46, min_samples_leaf=2, min_samples_split=2, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=15, max_features=26, max_leaf_nodes=46, min_samples_leaf=2, min_samples_split=2, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=15, max_features=26, max_leaf_nodes=46, min_samples_leaf=2, min_samples_split=2, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=15, max_features=26, max_leaf_nodes=46, min_samples_leaf=2, min_samples_split=2, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, max_features=24, max_leaf_nodes=28, min_samples_leaf=7, min_samples_split=20, n_estimators=120;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, max_features=24, max_leaf_nodes=28, min_samples_leaf=7, min_samples_split=20, n_estimators=120;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, max_features=24, max_leaf_nodes=28, min_samples_leaf=7, min_samples_split=20, n_estimators=120;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, max_features=24, max_leaf_nodes=28, min_samples_leaf=7, min_samples_split=20, n_estimators=120;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, max_features=24, max_leaf_nodes=28, min_samples_leaf=7, min_samples_split=20, n_estimators=120;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, max_features=12, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=3, n_estimators=140;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, max_features=12, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=3, n_estimators=140;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, max_features=12, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=3, n_estimators=140;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, max_features=12, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=3, n_estimators=140;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, max_features=12, max_leaf_nodes=48, min_samples_leaf=25, min_samples_split=3, n_estimators=140;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, max_features=18, max_leaf_nodes=33, min_samples_leaf=15, min_samples_split=17, n_estimators=52;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, max_features=18, max_leaf_nodes=33, min_samples_leaf=15, min_samples_split=17, n_estimators=52;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, max_features=18, max_leaf_nodes=33, min_samples_leaf=15, min_samples_split=17, n_estimators=52;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, max_features=18, max_leaf_nodes=33, min_samples_leaf=15, min_samples_split=17, n_estimators=52;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, max_features=18, max_leaf_nodes=33, min_samples_leaf=15, min_samples_split=17, n_estimators=52;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, max_features=23, max_leaf_nodes=33, min_samples_leaf=22, min_samples_split=13, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, max_features=23, max_leaf_nodes=33, min_samples_leaf=22, min_samples_split=13, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, max_features=23, max_leaf_nodes=33, min_samples_leaf=22, min_samples_split=13, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, max_features=23, max_leaf_nodes=33, min_samples_leaf=22, min_samples_split=13, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, max_features=23, max_leaf_nodes=33, min_samples_leaf=22, min_samples_split=13, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, max_features=16, max_leaf_nodes=42, min_samples_leaf=6, min_samples_split=19, n_estimators=8;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, max_features=16, max_leaf_nodes=42, min_samples_leaf=6, min_samples_split=19, n_estimators=8;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, max_features=16, max_leaf_nodes=42, min_samples_leaf=6, min_samples_split=19, n_estimators=8;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, max_features=16, max_leaf_nodes=42, min_samples_leaf=6, min_samples_split=19, n_estimators=8;, score=0.733 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, max_features=16, max_leaf_nodes=42, min_samples_leaf=6, min_samples_split=19, n_estimators=8;, score=0.797 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=11, max_features=20, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=14, n_estimators=162;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=11, max_features=20, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=14, n_estimators=162;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=11, max_features=20, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=14, n_estimators=162;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=11, max_features=20, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=14, n_estimators=162;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=11, max_features=20, max_leaf_nodes=32, min_samples_leaf=4, min_samples_split=14, n_estimators=162;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, max_features=16, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=3, n_estimators=21;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, max_features=16, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=3, n_estimators=21;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, max_features=16, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=3, n_estimators=21;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, max_features=16, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=3, n_estimators=21;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, max_features=16, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=3, n_estimators=21;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=11, max_leaf_nodes=31, min_samples_leaf=8, min_samples_split=20, n_estimators=1;, score=0.533 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=11, max_leaf_nodes=31, min_samples_leaf=8, min_samples_split=20, n_estimators=1;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=11, max_leaf_nodes=31, min_samples_leaf=8, min_samples_split=20, n_estimators=1;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=11, max_leaf_nodes=31, min_samples_leaf=8, min_samples_split=20, n_estimators=1;, score=0.617 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=11, max_leaf_nodes=31, min_samples_leaf=8, min_samples_split=20, n_estimators=1;, score=0.678 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, max_features=17, max_leaf_nodes=30, min_samples_leaf=9, min_samples_split=12, n_estimators=125;, score=0.783 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, max_features=17, max_leaf_nodes=30, min_samples_leaf=9, min_samples_split=12, n_estimators=125;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, max_features=17, max_leaf_nodes=30, min_samples_leaf=9, min_samples_split=12, n_estimators=125;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, max_features=17, max_leaf_nodes=30, min_samples_leaf=9, min_samples_split=12, n_estimators=125;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, max_features=17, max_leaf_nodes=30, min_samples_leaf=9, min_samples_split=12, n_estimators=125;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=19, max_leaf_nodes=38, min_samples_leaf=9, min_samples_split=19, n_estimators=96;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=19, max_leaf_nodes=38, min_samples_leaf=9, min_samples_split=19, n_estimators=96;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=19, max_leaf_nodes=38, min_samples_leaf=9, min_samples_split=19, n_estimators=96;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=19, max_leaf_nodes=38, min_samples_leaf=9, min_samples_split=19, n_estimators=96;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=19, max_leaf_nodes=38, min_samples_leaf=9, min_samples_split=19, n_estimators=96;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, max_features=18, max_leaf_nodes=48, min_samples_leaf=1, min_samples_split=9, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, max_features=18, max_leaf_nodes=48, min_samples_leaf=1, min_samples_split=9, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, max_features=18, max_leaf_nodes=48, min_samples_leaf=1, min_samples_split=9, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, max_features=18, max_leaf_nodes=48, min_samples_leaf=1, min_samples_split=9, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, max_features=18, max_leaf_nodes=48, min_samples_leaf=1, min_samples_split=9, n_estimators=132;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=28, min_samples_split=19, n_estimators=150;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=28, min_samples_split=19, n_estimators=150;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=28, min_samples_split=19, n_estimators=150;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=28, min_samples_split=19, n_estimators=150;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=28, min_samples_split=19, n_estimators=150;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, max_features=29, max_leaf_nodes=44, min_samples_leaf=14, min_samples_split=3, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, max_features=29, max_leaf_nodes=44, min_samples_leaf=14, min_samples_split=3, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, max_features=29, max_leaf_nodes=44, min_samples_leaf=14, min_samples_split=3, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, max_features=29, max_leaf_nodes=44, min_samples_leaf=14, min_samples_split=3, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, max_features=29, max_leaf_nodes=44, min_samples_leaf=14, min_samples_split=3, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=47, min_samples_leaf=24, min_samples_split=6, n_estimators=80;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=47, min_samples_leaf=24, min_samples_split=6, n_estimators=80;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=47, min_samples_leaf=24, min_samples_split=6, n_estimators=80;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=47, min_samples_leaf=24, min_samples_split=6, n_estimators=80;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=23, max_leaf_nodes=47, min_samples_leaf=24, min_samples_split=6, n_estimators=80;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=23, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=23, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=23, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=23, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=23, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=12, n_estimators=196;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, max_features=15, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=6, n_estimators=170;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, max_features=15, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=6, n_estimators=170;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, max_features=15, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=6, n_estimators=170;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, max_features=15, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=6, n_estimators=170;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, max_features=15, max_leaf_nodes=48, min_samples_leaf=12, min_samples_split=6, n_estimators=170;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=46, min_samples_leaf=19, min_samples_split=9, n_estimators=193;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=46, min_samples_leaf=19, min_samples_split=9, n_estimators=193;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=46, min_samples_leaf=19, min_samples_split=9, n_estimators=193;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=46, min_samples_leaf=19, min_samples_split=9, n_estimators=193;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=46, min_samples_leaf=19, min_samples_split=9, n_estimators=193;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, max_features=24, max_leaf_nodes=41, min_samples_leaf=21, min_samples_split=16, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, max_features=24, max_leaf_nodes=41, min_samples_leaf=21, min_samples_split=16, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, max_features=24, max_leaf_nodes=41, min_samples_leaf=21, min_samples_split=16, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, max_features=24, max_leaf_nodes=41, min_samples_leaf=21, min_samples_split=16, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, max_features=24, max_leaf_nodes=41, min_samples_leaf=21, min_samples_split=16, n_estimators=180;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, max_features=27, max_leaf_nodes=39, min_samples_leaf=23, min_samples_split=7, n_estimators=94;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, max_features=27, max_leaf_nodes=39, min_samples_leaf=23, min_samples_split=7, n_estimators=94;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, max_features=27, max_leaf_nodes=39, min_samples_leaf=23, min_samples_split=7, n_estimators=94;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, max_features=27, max_leaf_nodes=39, min_samples_leaf=23, min_samples_split=7, n_estimators=94;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, max_features=27, max_leaf_nodes=39, min_samples_leaf=23, min_samples_split=7, n_estimators=94;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=33, min_samples_leaf=6, min_samples_split=17, n_estimators=57;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=33, min_samples_leaf=6, min_samples_split=17, n_estimators=57;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=33, min_samples_leaf=6, min_samples_split=17, n_estimators=57;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=33, min_samples_leaf=6, min_samples_split=17, n_estimators=57;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=33, min_samples_leaf=6, min_samples_split=17, n_estimators=57;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=26, min_samples_leaf=4, min_samples_split=12, n_estimators=139;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=26, min_samples_leaf=4, min_samples_split=12, n_estimators=139;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=26, min_samples_leaf=4, min_samples_split=12, n_estimators=139;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=26, min_samples_leaf=4, min_samples_split=12, n_estimators=139;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=26, min_samples_leaf=4, min_samples_split=12, n_estimators=139;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=16, max_leaf_nodes=43, min_samples_leaf=21, min_samples_split=20, n_estimators=35;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=16, max_leaf_nodes=43, min_samples_leaf=21, min_samples_split=20, n_estimators=35;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=16, max_leaf_nodes=43, min_samples_leaf=21, min_samples_split=20, n_estimators=35;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=16, max_leaf_nodes=43, min_samples_leaf=21, min_samples_split=20, n_estimators=35;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=16, max_leaf_nodes=43, min_samples_leaf=21, min_samples_split=20, n_estimators=35;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, max_features=17, max_leaf_nodes=34, min_samples_leaf=13, min_samples_split=5, n_estimators=115;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, max_features=17, max_leaf_nodes=34, min_samples_leaf=13, min_samples_split=5, n_estimators=115;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, max_features=17, max_leaf_nodes=34, min_samples_leaf=13, min_samples_split=5, n_estimators=115;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, max_features=17, max_leaf_nodes=34, min_samples_leaf=13, min_samples_split=5, n_estimators=115;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, max_features=17, max_leaf_nodes=34, min_samples_leaf=13, min_samples_split=5, n_estimators=115;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, max_features=24, max_leaf_nodes=31, min_samples_leaf=10, min_samples_split=16, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, max_features=24, max_leaf_nodes=31, min_samples_leaf=10, min_samples_split=16, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, max_features=24, max_leaf_nodes=31, min_samples_leaf=10, min_samples_split=16, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, max_features=24, max_leaf_nodes=31, min_samples_leaf=10, min_samples_split=16, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, max_features=24, max_leaf_nodes=31, min_samples_leaf=10, min_samples_split=16, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, max_features=20, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=15, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, max_features=20, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=15, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, max_features=20, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=15, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, max_features=20, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=15, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, max_features=20, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=15, n_estimators=88;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=8, n_estimators=184;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=8, n_estimators=184;, score=0.783 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=8, n_estimators=184;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=8, n_estimators=184;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=8, n_estimators=184;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, max_features=19, max_leaf_nodes=34, min_samples_leaf=14, min_samples_split=4, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, max_features=19, max_leaf_nodes=34, min_samples_leaf=14, min_samples_split=4, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, max_features=19, max_leaf_nodes=34, min_samples_leaf=14, min_samples_split=4, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, max_features=19, max_leaf_nodes=34, min_samples_leaf=14, min_samples_split=4, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, max_features=19, max_leaf_nodes=34, min_samples_leaf=14, min_samples_split=4, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=18, max_leaf_nodes=33, min_samples_leaf=29, min_samples_split=6, n_estimators=31;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=18, max_leaf_nodes=33, min_samples_leaf=29, min_samples_split=6, n_estimators=31;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=18, max_leaf_nodes=33, min_samples_leaf=29, min_samples_split=6, n_estimators=31;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=18, max_leaf_nodes=33, min_samples_leaf=29, min_samples_split=6, n_estimators=31;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=18, max_leaf_nodes=33, min_samples_leaf=29, min_samples_split=6, n_estimators=31;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=47, min_samples_leaf=1, min_samples_split=18, n_estimators=45;, score=0.783 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=47, min_samples_leaf=1, min_samples_split=18, n_estimators=45;, score=0.767 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=47, min_samples_leaf=1, min_samples_split=18, n_estimators=45;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=47, min_samples_leaf=1, min_samples_split=18, n_estimators=45;, score=0.817 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=47, min_samples_leaf=1, min_samples_split=18, n_estimators=45;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, max_features=27, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=20, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, max_features=27, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=20, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, max_features=27, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=20, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, max_features=27, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=20, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, max_features=27, max_leaf_nodes=42, min_samples_leaf=23, min_samples_split=20, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, max_features=26, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=6, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, max_features=26, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=6, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, max_features=26, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=6, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, max_features=26, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=6, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, max_features=26, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=6, n_estimators=98;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, max_features=28, max_leaf_nodes=31, min_samples_leaf=1, min_samples_split=4, n_estimators=175;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, max_features=28, max_leaf_nodes=31, min_samples_leaf=1, min_samples_split=4, n_estimators=175;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, max_features=28, max_leaf_nodes=31, min_samples_leaf=1, min_samples_split=4, n_estimators=175;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, max_features=28, max_leaf_nodes=31, min_samples_leaf=1, min_samples_split=4, n_estimators=175;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, max_features=28, max_leaf_nodes=31, min_samples_leaf=1, min_samples_split=4, n_estimators=175;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, max_features=11, max_leaf_nodes=41, min_samples_leaf=19, min_samples_split=19, n_estimators=12;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, max_features=11, max_leaf_nodes=41, min_samples_leaf=19, min_samples_split=19, n_estimators=12;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, max_features=11, max_leaf_nodes=41, min_samples_leaf=19, min_samples_split=19, n_estimators=12;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, max_features=11, max_leaf_nodes=41, min_samples_leaf=19, min_samples_split=19, n_estimators=12;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, max_features=11, max_leaf_nodes=41, min_samples_leaf=19, min_samples_split=19, n_estimators=12;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, max_features=21, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=4, n_estimators=14;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, max_features=21, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=4, n_estimators=14;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, max_features=21, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=4, n_estimators=14;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, max_features=21, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=4, n_estimators=14;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, max_features=21, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=4, n_estimators=14;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=11, max_features=5, max_leaf_nodes=28, min_samples_leaf=22, min_samples_split=8, n_estimators=152;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=11, max_features=5, max_leaf_nodes=28, min_samples_leaf=22, min_samples_split=8, n_estimators=152;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=11, max_features=5, max_leaf_nodes=28, min_samples_leaf=22, min_samples_split=8, n_estimators=152;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=11, max_features=5, max_leaf_nodes=28, min_samples_leaf=22, min_samples_split=8, n_estimators=152;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=11, max_features=5, max_leaf_nodes=28, min_samples_leaf=22, min_samples_split=8, n_estimators=152;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=19, max_features=9, max_leaf_nodes=34, min_samples_leaf=4, min_samples_split=13, n_estimators=33;, score=0.767 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=19, max_features=9, max_leaf_nodes=34, min_samples_leaf=4, min_samples_split=13, n_estimators=33;, score=0.767 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=19, max_features=9, max_leaf_nodes=34, min_samples_leaf=4, min_samples_split=13, n_estimators=33;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=19, max_features=9, max_leaf_nodes=34, min_samples_leaf=4, min_samples_split=13, n_estimators=33;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=19, max_features=9, max_leaf_nodes=34, min_samples_leaf=4, min_samples_split=13, n_estimators=33;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=11, max_features=29, max_leaf_nodes=39, min_samples_leaf=25, min_samples_split=15, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=11, max_features=29, max_leaf_nodes=39, min_samples_leaf=25, min_samples_split=15, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=11, max_features=29, max_leaf_nodes=39, min_samples_leaf=25, min_samples_split=15, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=11, max_features=29, max_leaf_nodes=39, min_samples_leaf=25, min_samples_split=15, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=11, max_features=29, max_leaf_nodes=39, min_samples_leaf=25, min_samples_split=15, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=26, min_samples_leaf=27, min_samples_split=3, n_estimators=186;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=26, min_samples_leaf=27, min_samples_split=3, n_estimators=186;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=26, min_samples_leaf=27, min_samples_split=3, n_estimators=186;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=26, min_samples_leaf=27, min_samples_split=3, n_estimators=186;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=26, min_samples_leaf=27, min_samples_split=3, n_estimators=186;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, max_features=14, max_leaf_nodes=39, min_samples_leaf=1, min_samples_split=5, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, max_features=14, max_leaf_nodes=39, min_samples_leaf=1, min_samples_split=5, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, max_features=14, max_leaf_nodes=39, min_samples_leaf=1, min_samples_split=5, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, max_features=14, max_leaf_nodes=39, min_samples_leaf=1, min_samples_split=5, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, max_features=14, max_leaf_nodes=39, min_samples_leaf=1, min_samples_split=5, n_estimators=40;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=12, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=21, n_estimators=72;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=12, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=21, n_estimators=72;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=12, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=21, n_estimators=72;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=12, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=21, n_estimators=72;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=12, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=21, n_estimators=72;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, max_features=15, max_leaf_nodes=41, min_samples_leaf=24, min_samples_split=11, n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, max_features=15, max_leaf_nodes=41, min_samples_leaf=24, min_samples_split=11, n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, max_features=15, max_leaf_nodes=41, min_samples_leaf=24, min_samples_split=11, n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, max_features=15, max_leaf_nodes=41, min_samples_leaf=24, min_samples_split=11, n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, max_features=15, max_leaf_nodes=41, min_samples_leaf=24, min_samples_split=11, n_estimators=100;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=34, min_samples_leaf=12, min_samples_split=10, n_estimators=179;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=34, min_samples_leaf=12, min_samples_split=10, n_estimators=179;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=34, min_samples_leaf=12, min_samples_split=10, n_estimators=179;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=34, min_samples_leaf=12, min_samples_split=10, n_estimators=179;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=9, max_leaf_nodes=34, min_samples_leaf=12, min_samples_split=10, n_estimators=179;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, max_features=12, max_leaf_nodes=34, min_samples_leaf=16, min_samples_split=3, n_estimators=31;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, max_features=12, max_leaf_nodes=34, min_samples_leaf=16, min_samples_split=3, n_estimators=31;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, max_features=12, max_leaf_nodes=34, min_samples_leaf=16, min_samples_split=3, n_estimators=31;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, max_features=12, max_leaf_nodes=34, min_samples_leaf=16, min_samples_split=3, n_estimators=31;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, max_features=12, max_leaf_nodes=34, min_samples_leaf=16, min_samples_split=3, n_estimators=31;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=37, min_samples_leaf=14, min_samples_split=8, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=37, min_samples_leaf=14, min_samples_split=8, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=37, min_samples_leaf=14, min_samples_split=8, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=37, min_samples_leaf=14, min_samples_split=8, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, max_features=18, max_leaf_nodes=37, min_samples_leaf=14, min_samples_split=8, n_estimators=46;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, max_features=15, max_leaf_nodes=29, min_samples_leaf=18, min_samples_split=13, n_estimators=12;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, max_features=15, max_leaf_nodes=29, min_samples_leaf=18, min_samples_split=13, n_estimators=12;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, max_features=15, max_leaf_nodes=29, min_samples_leaf=18, min_samples_split=13, n_estimators=12;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, max_features=15, max_leaf_nodes=29, min_samples_leaf=18, min_samples_split=13, n_estimators=12;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, max_features=15, max_leaf_nodes=29, min_samples_leaf=18, min_samples_split=13, n_estimators=12;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, max_features=22, max_leaf_nodes=45, min_samples_leaf=10, min_samples_split=12, n_estimators=168;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, max_features=22, max_leaf_nodes=45, min_samples_leaf=10, min_samples_split=12, n_estimators=168;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, max_features=22, max_leaf_nodes=45, min_samples_leaf=10, min_samples_split=12, n_estimators=168;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, max_features=22, max_leaf_nodes=45, min_samples_leaf=10, min_samples_split=12, n_estimators=168;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, max_features=22, max_leaf_nodes=45, min_samples_leaf=10, min_samples_split=12, n_estimators=168;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=11, max_features=19, max_leaf_nodes=34, min_samples_leaf=24, min_samples_split=8, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=11, max_features=19, max_leaf_nodes=34, min_samples_leaf=24, min_samples_split=8, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=11, max_features=19, max_leaf_nodes=34, min_samples_leaf=24, min_samples_split=8, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=11, max_features=19, max_leaf_nodes=34, min_samples_leaf=24, min_samples_split=8, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=11, max_features=19, max_leaf_nodes=34, min_samples_leaf=24, min_samples_split=8, n_estimators=123;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=37, min_samples_leaf=15, min_samples_split=9, n_estimators=60;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=37, min_samples_leaf=15, min_samples_split=9, n_estimators=60;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=37, min_samples_leaf=15, min_samples_split=9, n_estimators=60;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=37, min_samples_leaf=15, min_samples_split=9, n_estimators=60;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, max_features=19, max_leaf_nodes=37, min_samples_leaf=15, min_samples_split=9, n_estimators=60;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=12, max_features=29, max_leaf_nodes=34, min_samples_leaf=8, min_samples_split=15, n_estimators=25;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=12, max_features=29, max_leaf_nodes=34, min_samples_leaf=8, min_samples_split=15, n_estimators=25;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=12, max_features=29, max_leaf_nodes=34, min_samples_leaf=8, min_samples_split=15, n_estimators=25;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=12, max_features=29, max_leaf_nodes=34, min_samples_leaf=8, min_samples_split=15, n_estimators=25;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=12, max_features=29, max_leaf_nodes=34, min_samples_leaf=8, min_samples_split=15, n_estimators=25;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=28, max_leaf_nodes=26, min_samples_leaf=1, min_samples_split=6, n_estimators=55;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=28, max_leaf_nodes=26, min_samples_leaf=1, min_samples_split=6, n_estimators=55;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=28, max_leaf_nodes=26, min_samples_leaf=1, min_samples_split=6, n_estimators=55;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=28, max_leaf_nodes=26, min_samples_leaf=1, min_samples_split=6, n_estimators=55;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=28, max_leaf_nodes=26, min_samples_leaf=1, min_samples_split=6, n_estimators=55;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, max_features=27, max_leaf_nodes=37, min_samples_leaf=8, min_samples_split=8, n_estimators=67;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, max_features=27, max_leaf_nodes=37, min_samples_leaf=8, min_samples_split=8, n_estimators=67;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, max_features=27, max_leaf_nodes=37, min_samples_leaf=8, min_samples_split=8, n_estimators=67;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, max_features=27, max_leaf_nodes=37, min_samples_leaf=8, min_samples_split=8, n_estimators=67;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, max_features=27, max_leaf_nodes=37, min_samples_leaf=8, min_samples_split=8, n_estimators=67;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, max_features=28, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=15, n_estimators=144;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, max_features=28, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=15, n_estimators=144;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, max_features=28, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=15, n_estimators=144;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, max_features=28, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=15, n_estimators=144;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, max_features=28, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=15, n_estimators=144;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, max_features=11, max_leaf_nodes=40, min_samples_leaf=19, min_samples_split=19, n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, max_features=11, max_leaf_nodes=40, min_samples_leaf=19, min_samples_split=19, n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, max_features=11, max_leaf_nodes=40, min_samples_leaf=19, min_samples_split=19, n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, max_features=11, max_leaf_nodes=40, min_samples_leaf=19, min_samples_split=19, n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, max_features=11, max_leaf_nodes=40, min_samples_leaf=19, min_samples_split=19, n_estimators=100;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, max_features=23, max_leaf_nodes=46, min_samples_leaf=1, min_samples_split=7, n_estimators=117;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, max_features=23, max_leaf_nodes=46, min_samples_leaf=1, min_samples_split=7, n_estimators=117;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, max_features=23, max_leaf_nodes=46, min_samples_leaf=1, min_samples_split=7, n_estimators=117;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, max_features=23, max_leaf_nodes=46, min_samples_leaf=1, min_samples_split=7, n_estimators=117;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, max_features=23, max_leaf_nodes=46, min_samples_leaf=1, min_samples_split=7, n_estimators=117;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=39, min_samples_leaf=5, min_samples_split=5, n_estimators=76;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=39, min_samples_leaf=5, min_samples_split=5, n_estimators=76;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=39, min_samples_leaf=5, min_samples_split=5, n_estimators=76;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=39, min_samples_leaf=5, min_samples_split=5, n_estimators=76;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=39, min_samples_leaf=5, min_samples_split=5, n_estimators=76;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=14, max_leaf_nodes=30, min_samples_leaf=2, min_samples_split=7, n_estimators=189;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=14, max_leaf_nodes=30, min_samples_leaf=2, min_samples_split=7, n_estimators=189;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=14, max_leaf_nodes=30, min_samples_leaf=2, min_samples_split=7, n_estimators=189;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=14, max_leaf_nodes=30, min_samples_leaf=2, min_samples_split=7, n_estimators=189;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=14, max_leaf_nodes=30, min_samples_leaf=2, min_samples_split=7, n_estimators=189;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=17, max_leaf_nodes=37, min_samples_leaf=28, min_samples_split=13, n_estimators=197;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=17, max_leaf_nodes=37, min_samples_leaf=28, min_samples_split=13, n_estimators=197;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=17, max_leaf_nodes=37, min_samples_leaf=28, min_samples_split=13, n_estimators=197;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=17, max_leaf_nodes=37, min_samples_leaf=28, min_samples_split=13, n_estimators=197;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=17, max_leaf_nodes=37, min_samples_leaf=28, min_samples_split=13, n_estimators=197;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=11;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=11;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=11;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=11;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, max_features=20, max_leaf_nodes=49, min_samples_leaf=12, min_samples_split=14, n_estimators=11;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=17, max_features=11, max_leaf_nodes=30, min_samples_leaf=22, min_samples_split=18, n_estimators=91;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=17, max_features=11, max_leaf_nodes=30, min_samples_leaf=22, min_samples_split=18, n_estimators=91;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=17, max_features=11, max_leaf_nodes=30, min_samples_leaf=22, min_samples_split=18, n_estimators=91;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=17, max_features=11, max_leaf_nodes=30, min_samples_leaf=22, min_samples_split=18, n_estimators=91;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=17, max_features=11, max_leaf_nodes=30, min_samples_leaf=22, min_samples_split=18, n_estimators=91;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=6, n_estimators=80;, score=0.767 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=6, n_estimators=80;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=6, n_estimators=80;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=6, n_estimators=80;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=6, n_estimators=80;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=43, min_samples_leaf=11, min_samples_split=18, n_estimators=117;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=43, min_samples_leaf=11, min_samples_split=18, n_estimators=117;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=43, min_samples_leaf=11, min_samples_split=18, n_estimators=117;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=43, min_samples_leaf=11, min_samples_split=18, n_estimators=117;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=16, max_leaf_nodes=43, min_samples_leaf=11, min_samples_split=18, n_estimators=117;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=13, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=11, n_estimators=196;, score=0.783 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=13, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=11, n_estimators=196;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=13, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=11, n_estimators=196;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=13, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=11, n_estimators=196;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=13, max_leaf_nodes=38, min_samples_leaf=5, min_samples_split=11, n_estimators=196;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=17, max_leaf_nodes=30, min_samples_leaf=6, min_samples_split=12, n_estimators=151;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=17, max_leaf_nodes=30, min_samples_leaf=6, min_samples_split=12, n_estimators=151;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=17, max_leaf_nodes=30, min_samples_leaf=6, min_samples_split=12, n_estimators=151;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=17, max_leaf_nodes=30, min_samples_leaf=6, min_samples_split=12, n_estimators=151;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=17, max_leaf_nodes=30, min_samples_leaf=6, min_samples_split=12, n_estimators=151;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, max_features=29, max_leaf_nodes=25, min_samples_leaf=19, min_samples_split=17, n_estimators=163;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, max_features=29, max_leaf_nodes=25, min_samples_leaf=19, min_samples_split=17, n_estimators=163;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, max_features=29, max_leaf_nodes=25, min_samples_leaf=19, min_samples_split=17, n_estimators=163;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, max_features=29, max_leaf_nodes=25, min_samples_leaf=19, min_samples_split=17, n_estimators=163;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, max_features=29, max_leaf_nodes=25, min_samples_leaf=19, min_samples_split=17, n_estimators=163;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=39, min_samples_leaf=7, min_samples_split=12, n_estimators=51;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=39, min_samples_leaf=7, min_samples_split=12, n_estimators=51;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=39, min_samples_leaf=7, min_samples_split=12, n_estimators=51;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=39, min_samples_leaf=7, min_samples_split=12, n_estimators=51;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, max_features=18, max_leaf_nodes=39, min_samples_leaf=7, min_samples_split=12, n_estimators=51;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=7, max_leaf_nodes=36, min_samples_leaf=1, min_samples_split=14, n_estimators=30;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=7, max_leaf_nodes=36, min_samples_leaf=1, min_samples_split=14, n_estimators=30;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=7, max_leaf_nodes=36, min_samples_leaf=1, min_samples_split=14, n_estimators=30;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=7, max_leaf_nodes=36, min_samples_leaf=1, min_samples_split=14, n_estimators=30;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=7, max_leaf_nodes=36, min_samples_leaf=1, min_samples_split=14, n_estimators=30;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=13, max_features=16, max_leaf_nodes=35, min_samples_leaf=21, min_samples_split=10, n_estimators=31;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=13, max_features=16, max_leaf_nodes=35, min_samples_leaf=21, min_samples_split=10, n_estimators=31;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=13, max_features=16, max_leaf_nodes=35, min_samples_leaf=21, min_samples_split=10, n_estimators=31;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=13, max_features=16, max_leaf_nodes=35, min_samples_leaf=21, min_samples_split=10, n_estimators=31;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=13, max_features=16, max_leaf_nodes=35, min_samples_leaf=21, min_samples_split=10, n_estimators=31;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=6, max_leaf_nodes=45, min_samples_leaf=14, min_samples_split=18, n_estimators=185;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=6, max_leaf_nodes=45, min_samples_leaf=14, min_samples_split=18, n_estimators=185;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=6, max_leaf_nodes=45, min_samples_leaf=14, min_samples_split=18, n_estimators=185;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=6, max_leaf_nodes=45, min_samples_leaf=14, min_samples_split=18, n_estimators=185;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=6, max_leaf_nodes=45, min_samples_leaf=14, min_samples_split=18, n_estimators=185;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=18, max_features=13, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=5, n_estimators=134;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=18, max_features=13, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=5, n_estimators=134;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=18, max_features=13, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=5, n_estimators=134;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=18, max_features=13, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=5, n_estimators=134;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=18, max_features=13, max_leaf_nodes=49, min_samples_leaf=19, min_samples_split=5, n_estimators=134;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, max_features=21, max_leaf_nodes=44, min_samples_leaf=20, min_samples_split=11, n_estimators=140;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, max_features=21, max_leaf_nodes=44, min_samples_leaf=20, min_samples_split=11, n_estimators=140;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, max_features=21, max_leaf_nodes=44, min_samples_leaf=20, min_samples_split=11, n_estimators=140;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, max_features=21, max_leaf_nodes=44, min_samples_leaf=20, min_samples_split=11, n_estimators=140;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, max_features=21, max_leaf_nodes=44, min_samples_leaf=20, min_samples_split=11, n_estimators=140;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=22, max_leaf_nodes=38, min_samples_leaf=8, min_samples_split=18, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=22, max_leaf_nodes=38, min_samples_leaf=8, min_samples_split=18, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=22, max_leaf_nodes=38, min_samples_leaf=8, min_samples_split=18, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=22, max_leaf_nodes=38, min_samples_leaf=8, min_samples_split=18, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=22, max_leaf_nodes=38, min_samples_leaf=8, min_samples_split=18, n_estimators=116;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, max_features=28, max_leaf_nodes=35, min_samples_leaf=1, min_samples_split=20, n_estimators=194;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, max_features=28, max_leaf_nodes=35, min_samples_leaf=1, min_samples_split=20, n_estimators=194;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, max_features=28, max_leaf_nodes=35, min_samples_leaf=1, min_samples_split=20, n_estimators=194;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, max_features=28, max_leaf_nodes=35, min_samples_leaf=1, min_samples_split=20, n_estimators=194;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, max_features=28, max_leaf_nodes=35, min_samples_leaf=1, min_samples_split=20, n_estimators=194;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=3, n_estimators=94;, score=0.767 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=3, n_estimators=94;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=3, n_estimators=94;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=3, n_estimators=94;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=7, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=3, n_estimators=94;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, max_features=23, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=19, n_estimators=27;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, max_features=23, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=19, n_estimators=27;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, max_features=23, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=19, n_estimators=27;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, max_features=23, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=19, n_estimators=27;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, max_features=23, max_leaf_nodes=25, min_samples_leaf=14, min_samples_split=19, n_estimators=27;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, max_features=15, max_leaf_nodes=29, min_samples_leaf=19, min_samples_split=11, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, max_features=15, max_leaf_nodes=29, min_samples_leaf=19, min_samples_split=11, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, max_features=15, max_leaf_nodes=29, min_samples_leaf=19, min_samples_split=11, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, max_features=15, max_leaf_nodes=29, min_samples_leaf=19, min_samples_split=11, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, max_features=15, max_leaf_nodes=29, min_samples_leaf=19, min_samples_split=11, n_estimators=40;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, max_features=22, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=20, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, max_features=22, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=20, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, max_features=22, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=20, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, max_features=22, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=20, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, max_features=22, max_leaf_nodes=34, min_samples_leaf=2, min_samples_split=20, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=14, max_leaf_nodes=38, min_samples_leaf=7, min_samples_split=16, n_estimators=17;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=14, max_leaf_nodes=38, min_samples_leaf=7, min_samples_split=16, n_estimators=17;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=14, max_leaf_nodes=38, min_samples_leaf=7, min_samples_split=16, n_estimators=17;, score=0.783 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=14, max_leaf_nodes=38, min_samples_leaf=7, min_samples_split=16, n_estimators=17;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=14, max_leaf_nodes=38, min_samples_leaf=7, min_samples_split=16, n_estimators=17;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, max_features=15, max_leaf_nodes=35, min_samples_leaf=13, min_samples_split=15, n_estimators=179;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, max_features=15, max_leaf_nodes=35, min_samples_leaf=13, min_samples_split=15, n_estimators=179;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, max_features=15, max_leaf_nodes=35, min_samples_leaf=13, min_samples_split=15, n_estimators=179;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, max_features=15, max_leaf_nodes=35, min_samples_leaf=13, min_samples_split=15, n_estimators=179;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, max_features=15, max_leaf_nodes=35, min_samples_leaf=13, min_samples_split=15, n_estimators=179;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=11, max_features=21, max_leaf_nodes=45, min_samples_leaf=22, min_samples_split=18, n_estimators=112;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=11, max_features=21, max_leaf_nodes=45, min_samples_leaf=22, min_samples_split=18, n_estimators=112;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=11, max_features=21, max_leaf_nodes=45, min_samples_leaf=22, min_samples_split=18, n_estimators=112;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=11, max_features=21, max_leaf_nodes=45, min_samples_leaf=22, min_samples_split=18, n_estimators=112;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=11, max_features=21, max_leaf_nodes=45, min_samples_leaf=22, min_samples_split=18, n_estimators=112;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=45, min_samples_leaf=23, min_samples_split=9, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=45, min_samples_leaf=23, min_samples_split=9, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=45, min_samples_leaf=23, min_samples_split=9, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=45, min_samples_leaf=23, min_samples_split=9, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=7, max_features=21, max_leaf_nodes=45, min_samples_leaf=23, min_samples_split=9, n_estimators=143;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=13, n_estimators=37;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=13, n_estimators=37;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=13, n_estimators=37;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=13, n_estimators=37;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=26, min_samples_leaf=23, min_samples_split=13, n_estimators=37;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=9, min_samples_split=11, n_estimators=191;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=9, min_samples_split=11, n_estimators=191;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=9, min_samples_split=11, n_estimators=191;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=9, min_samples_split=11, n_estimators=191;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=12, max_features=8, max_leaf_nodes=44, min_samples_leaf=9, min_samples_split=11, n_estimators=191;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, max_features=12, max_leaf_nodes=48, min_samples_leaf=9, min_samples_split=3, n_estimators=42;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, max_features=12, max_leaf_nodes=48, min_samples_leaf=9, min_samples_split=3, n_estimators=42;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, max_features=12, max_leaf_nodes=48, min_samples_leaf=9, min_samples_split=3, n_estimators=42;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, max_features=12, max_leaf_nodes=48, min_samples_leaf=9, min_samples_split=3, n_estimators=42;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, max_features=12, max_leaf_nodes=48, min_samples_leaf=9, min_samples_split=3, n_estimators=42;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=6, max_leaf_nodes=27, min_samples_leaf=9, min_samples_split=10, n_estimators=55;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=6, max_leaf_nodes=27, min_samples_leaf=9, min_samples_split=10, n_estimators=55;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=6, max_leaf_nodes=27, min_samples_leaf=9, min_samples_split=10, n_estimators=55;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=6, max_leaf_nodes=27, min_samples_leaf=9, min_samples_split=10, n_estimators=55;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=6, max_leaf_nodes=27, min_samples_leaf=9, min_samples_split=10, n_estimators=55;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=5, n_estimators=33;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=5, n_estimators=33;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=5, n_estimators=33;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=5, n_estimators=33;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, max_features=26, max_leaf_nodes=36, min_samples_leaf=12, min_samples_split=5, n_estimators=33;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=13, max_features=8, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=14, n_estimators=128;, score=0.783 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=13, max_features=8, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=14, n_estimators=128;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=13, max_features=8, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=14, n_estimators=128;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=13, max_features=8, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=14, n_estimators=128;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=13, max_features=8, max_leaf_nodes=46, min_samples_leaf=5, min_samples_split=14, n_estimators=128;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, max_features=14, max_leaf_nodes=39, min_samples_leaf=9, min_samples_split=12, n_estimators=153;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, max_features=14, max_leaf_nodes=39, min_samples_leaf=9, min_samples_split=12, n_estimators=153;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, max_features=14, max_leaf_nodes=39, min_samples_leaf=9, min_samples_split=12, n_estimators=153;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, max_features=14, max_leaf_nodes=39, min_samples_leaf=9, min_samples_split=12, n_estimators=153;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, max_features=14, max_leaf_nodes=39, min_samples_leaf=9, min_samples_split=12, n_estimators=153;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=5, n_estimators=69;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=5, n_estimators=69;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=5, n_estimators=69;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=5, n_estimators=69;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, max_features=9, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=5, n_estimators=69;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=10, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=15, n_estimators=190;, score=0.783 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=10, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=15, n_estimators=190;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=10, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=15, n_estimators=190;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=10, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=15, n_estimators=190;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=10, max_leaf_nodes=31, min_samples_leaf=4, min_samples_split=15, n_estimators=190;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=18, max_features=14, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=3, n_estimators=154;, score=0.783 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=18, max_features=14, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=3, n_estimators=154;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=18, max_features=14, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=3, n_estimators=154;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=18, max_features=14, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=3, n_estimators=154;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=18, max_features=14, max_leaf_nodes=46, min_samples_leaf=9, min_samples_split=3, n_estimators=154;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=41, min_samples_leaf=8, min_samples_split=6, n_estimators=122;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=41, min_samples_leaf=8, min_samples_split=6, n_estimators=122;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=41, min_samples_leaf=8, min_samples_split=6, n_estimators=122;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=41, min_samples_leaf=8, min_samples_split=6, n_estimators=122;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, max_features=28, max_leaf_nodes=41, min_samples_leaf=8, min_samples_split=6, n_estimators=122;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, max_features=29, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=18, n_estimators=187;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, max_features=29, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=18, n_estimators=187;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, max_features=29, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=18, n_estimators=187;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, max_features=29, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=18, n_estimators=187;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, max_features=29, max_leaf_nodes=30, min_samples_leaf=4, min_samples_split=18, n_estimators=187;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=8, max_features=29, max_leaf_nodes=37, min_samples_leaf=24, min_samples_split=20, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=8, max_features=29, max_leaf_nodes=37, min_samples_leaf=24, min_samples_split=20, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=8, max_features=29, max_leaf_nodes=37, min_samples_leaf=24, min_samples_split=20, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=8, max_features=29, max_leaf_nodes=37, min_samples_leaf=24, min_samples_split=20, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=8, max_features=29, max_leaf_nodes=37, min_samples_leaf=24, min_samples_split=20, n_estimators=189;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=12, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=14, n_estimators=151;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=12, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=14, n_estimators=151;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=12, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=14, n_estimators=151;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=12, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=14, n_estimators=151;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=12, max_leaf_nodes=35, min_samples_leaf=28, min_samples_split=14, n_estimators=151;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, max_features=25, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=10, n_estimators=54;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, max_features=25, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=10, n_estimators=54;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, max_features=25, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=10, n_estimators=54;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, max_features=25, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=10, n_estimators=54;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, max_features=25, max_leaf_nodes=32, min_samples_leaf=6, min_samples_split=10, n_estimators=54;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, max_features=10, max_leaf_nodes=47, min_samples_leaf=13, min_samples_split=21, n_estimators=22;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, max_features=10, max_leaf_nodes=47, min_samples_leaf=13, min_samples_split=21, n_estimators=22;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, max_features=10, max_leaf_nodes=47, min_samples_leaf=13, min_samples_split=21, n_estimators=22;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, max_features=10, max_leaf_nodes=47, min_samples_leaf=13, min_samples_split=21, n_estimators=22;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, max_features=10, max_leaf_nodes=47, min_samples_leaf=13, min_samples_split=21, n_estimators=22;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=17, max_features=24, max_leaf_nodes=29, min_samples_leaf=22, min_samples_split=20, n_estimators=191;, score=nan total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=17, max_features=24, max_leaf_nodes=29, min_samples_leaf=22, min_samples_split=20, n_estimators=191;, score=nan total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=17, max_features=24, max_leaf_nodes=29, min_samples_leaf=22, min_samples_split=20, n_estimators=191;, score=nan total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=17, max_features=24, max_leaf_nodes=29, min_samples_leaf=22, min_samples_split=20, n_estimators=191;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=17, max_features=24, max_leaf_nodes=29, min_samples_leaf=22, min_samples_split=20, n_estimators=191;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=28, max_leaf_nodes=45, min_samples_leaf=11, min_samples_split=11, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=28, max_leaf_nodes=45, min_samples_leaf=11, min_samples_split=11, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=28, max_leaf_nodes=45, min_samples_leaf=11, min_samples_split=11, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=28, max_leaf_nodes=45, min_samples_leaf=11, min_samples_split=11, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=28, max_leaf_nodes=45, min_samples_leaf=11, min_samples_split=11, n_estimators=142;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=13, max_leaf_nodes=43, min_samples_leaf=8, min_samples_split=16, n_estimators=28;, score=0.783 total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=13, max_leaf_nodes=43, min_samples_leaf=8, min_samples_split=16, n_estimators=28;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=13, max_leaf_nodes=43, min_samples_leaf=8, min_samples_split=16, n_estimators=28;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=13, max_leaf_nodes=43, min_samples_leaf=8, min_samples_split=16, n_estimators=28;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=13, max_leaf_nodes=43, min_samples_leaf=8, min_samples_split=16, n_estimators=28;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=13, max_features=22, max_leaf_nodes=31, min_samples_leaf=6, min_samples_split=6, n_estimators=183;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=13, max_features=22, max_leaf_nodes=31, min_samples_leaf=6, min_samples_split=6, n_estimators=183;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=13, max_features=22, max_leaf_nodes=31, min_samples_leaf=6, min_samples_split=6, n_estimators=183;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=13, max_features=22, max_leaf_nodes=31, min_samples_leaf=6, min_samples_split=6, n_estimators=183;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=13, max_features=22, max_leaf_nodes=31, min_samples_leaf=6, min_samples_split=6, n_estimators=183;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, max_features=8, max_leaf_nodes=30, min_samples_leaf=14, min_samples_split=19, n_estimators=107;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, max_features=8, max_leaf_nodes=30, min_samples_leaf=14, min_samples_split=19, n_estimators=107;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, max_features=8, max_leaf_nodes=30, min_samples_leaf=14, min_samples_split=19, n_estimators=107;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, max_features=8, max_leaf_nodes=30, min_samples_leaf=14, min_samples_split=19, n_estimators=107;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, max_features=8, max_leaf_nodes=30, min_samples_leaf=14, min_samples_split=19, n_estimators=107;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=27, min_samples_leaf=28, min_samples_split=16, n_estimators=35;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=27, min_samples_leaf=28, min_samples_split=16, n_estimators=35;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=27, min_samples_leaf=28, min_samples_split=16, n_estimators=35;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=27, min_samples_leaf=28, min_samples_split=16, n_estimators=35;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, max_features=25, max_leaf_nodes=27, min_samples_leaf=28, min_samples_split=16, n_estimators=35;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, max_features=11, max_leaf_nodes=30, min_samples_leaf=17, min_samples_split=3, n_estimators=27;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, max_features=11, max_leaf_nodes=30, min_samples_leaf=17, min_samples_split=3, n_estimators=27;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, max_features=11, max_leaf_nodes=30, min_samples_leaf=17, min_samples_split=3, n_estimators=27;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, max_features=11, max_leaf_nodes=30, min_samples_leaf=17, min_samples_split=3, n_estimators=27;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, max_features=11, max_leaf_nodes=30, min_samples_leaf=17, min_samples_split=3, n_estimators=27;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, max_features=21, max_leaf_nodes=34, min_samples_leaf=7, min_samples_split=17, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, max_features=21, max_leaf_nodes=34, min_samples_leaf=7, min_samples_split=17, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, max_features=21, max_leaf_nodes=34, min_samples_leaf=7, min_samples_split=17, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, max_features=21, max_leaf_nodes=34, min_samples_leaf=7, min_samples_split=17, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, max_features=21, max_leaf_nodes=34, min_samples_leaf=7, min_samples_split=17, n_estimators=22;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=25, min_samples_leaf=2, min_samples_split=21, n_estimators=35;, score=0.767 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=25, min_samples_leaf=2, min_samples_split=21, n_estimators=35;, score=0.783 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=25, min_samples_leaf=2, min_samples_split=21, n_estimators=35;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=25, min_samples_leaf=2, min_samples_split=21, n_estimators=35;, score=0.817 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=8, max_features=14, max_leaf_nodes=25, min_samples_leaf=2, min_samples_split=21, n_estimators=35;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, max_features=18, max_leaf_nodes=26, min_samples_leaf=2, min_samples_split=7, n_estimators=130;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, max_features=18, max_leaf_nodes=26, min_samples_leaf=2, min_samples_split=7, n_estimators=130;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, max_features=18, max_leaf_nodes=26, min_samples_leaf=2, min_samples_split=7, n_estimators=130;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, max_features=18, max_leaf_nodes=26, min_samples_leaf=2, min_samples_split=7, n_estimators=130;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, max_features=18, max_leaf_nodes=26, min_samples_leaf=2, min_samples_split=7, n_estimators=130;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, max_features=27, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=13, n_estimators=2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, max_features=27, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=13, n_estimators=2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, max_features=27, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=13, n_estimators=2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, max_features=27, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=13, n_estimators=2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, max_features=27, max_leaf_nodes=26, min_samples_leaf=3, min_samples_split=13, n_estimators=2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=12, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=2, n_estimators=167;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=12, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=2, n_estimators=167;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=12, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=2, n_estimators=167;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=12, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=2, n_estimators=167;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=12, max_leaf_nodes=42, min_samples_leaf=13, min_samples_split=2, n_estimators=167;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=23, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=10, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=23, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=10, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=23, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=10, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=23, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=10, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=23, max_leaf_nodes=43, min_samples_leaf=18, min_samples_split=10, n_estimators=186;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=7, max_features=18, max_leaf_nodes=49, min_samples_leaf=14, min_samples_split=10, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=7, max_features=18, max_leaf_nodes=49, min_samples_leaf=14, min_samples_split=10, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=7, max_features=18, max_leaf_nodes=49, min_samples_leaf=14, min_samples_split=10, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=7, max_features=18, max_leaf_nodes=49, min_samples_leaf=14, min_samples_split=10, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=7, max_features=18, max_leaf_nodes=49, min_samples_leaf=14, min_samples_split=10, n_estimators=192;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=13, max_features=9, max_leaf_nodes=32, min_samples_leaf=24, min_samples_split=10, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=13, max_features=9, max_leaf_nodes=32, min_samples_leaf=24, min_samples_split=10, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=13, max_features=9, max_leaf_nodes=32, min_samples_leaf=24, min_samples_split=10, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=13, max_features=9, max_leaf_nodes=32, min_samples_leaf=24, min_samples_split=10, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=13, max_features=9, max_leaf_nodes=32, min_samples_leaf=24, min_samples_split=10, n_estimators=130;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=15, max_features=11, max_leaf_nodes=46, min_samples_leaf=24, min_samples_split=12, n_estimators=113;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=15, max_features=11, max_leaf_nodes=46, min_samples_leaf=24, min_samples_split=12, n_estimators=113;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=15, max_features=11, max_leaf_nodes=46, min_samples_leaf=24, min_samples_split=12, n_estimators=113;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=15, max_features=11, max_leaf_nodes=46, min_samples_leaf=24, min_samples_split=12, n_estimators=113;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=15, max_features=11, max_leaf_nodes=46, min_samples_leaf=24, min_samples_split=12, n_estimators=113;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=11, max_features=11, max_leaf_nodes=42, min_samples_leaf=11, min_samples_split=18, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=11, max_features=11, max_leaf_nodes=42, min_samples_leaf=11, min_samples_split=18, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=11, max_features=11, max_leaf_nodes=42, min_samples_leaf=11, min_samples_split=18, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=11, max_features=11, max_leaf_nodes=42, min_samples_leaf=11, min_samples_split=18, n_estimators=40;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=11, max_features=11, max_leaf_nodes=42, min_samples_leaf=11, min_samples_split=18, n_estimators=40;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=47, min_samples_leaf=27, min_samples_split=5, n_estimators=114;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=47, min_samples_leaf=27, min_samples_split=5, n_estimators=114;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=47, min_samples_leaf=27, min_samples_split=5, n_estimators=114;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=47, min_samples_leaf=27, min_samples_split=5, n_estimators=114;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=14, max_features=8, max_leaf_nodes=47, min_samples_leaf=27, min_samples_split=5, n_estimators=114;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=12, n_estimators=105;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=12, n_estimators=105;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=12, n_estimators=105;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=12, n_estimators=105;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=18, max_leaf_nodes=45, min_samples_leaf=1, min_samples_split=12, n_estimators=105;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=6, n_estimators=165;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=6, n_estimators=165;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=6, n_estimators=165;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=6, n_estimators=165;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=14, max_features=7, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=6, n_estimators=165;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, max_features=24, max_leaf_nodes=37, min_samples_leaf=18, min_samples_split=19, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, max_features=24, max_leaf_nodes=37, min_samples_leaf=18, min_samples_split=19, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, max_features=24, max_leaf_nodes=37, min_samples_leaf=18, min_samples_split=19, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, max_features=24, max_leaf_nodes=37, min_samples_leaf=18, min_samples_split=19, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, max_features=24, max_leaf_nodes=37, min_samples_leaf=18, min_samples_split=19, n_estimators=104;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, max_features=7, max_leaf_nodes=27, min_samples_leaf=18, min_samples_split=13, n_estimators=192;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, max_features=7, max_leaf_nodes=27, min_samples_leaf=18, min_samples_split=13, n_estimators=192;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, max_features=7, max_leaf_nodes=27, min_samples_leaf=18, min_samples_split=13, n_estimators=192;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, max_features=7, max_leaf_nodes=27, min_samples_leaf=18, min_samples_split=13, n_estimators=192;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, max_features=7, max_leaf_nodes=27, min_samples_leaf=18, min_samples_split=13, n_estimators=192;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=29, max_leaf_nodes=35, min_samples_leaf=26, min_samples_split=3, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=29, max_leaf_nodes=35, min_samples_leaf=26, min_samples_split=3, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=29, max_leaf_nodes=35, min_samples_leaf=26, min_samples_split=3, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=29, max_leaf_nodes=35, min_samples_leaf=26, min_samples_split=3, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=29, max_leaf_nodes=35, min_samples_leaf=26, min_samples_split=3, n_estimators=170;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=20, max_leaf_nodes=27, min_samples_leaf=4, min_samples_split=7, n_estimators=188;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=20, max_leaf_nodes=27, min_samples_leaf=4, min_samples_split=7, n_estimators=188;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=20, max_leaf_nodes=27, min_samples_leaf=4, min_samples_split=7, n_estimators=188;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=20, max_leaf_nodes=27, min_samples_leaf=4, min_samples_split=7, n_estimators=188;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=20, max_leaf_nodes=27, min_samples_leaf=4, min_samples_split=7, n_estimators=188;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=16, max_features=28, max_leaf_nodes=28, min_samples_leaf=25, min_samples_split=16, n_estimators=34;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=16, max_features=28, max_leaf_nodes=28, min_samples_leaf=25, min_samples_split=16, n_estimators=34;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=16, max_features=28, max_leaf_nodes=28, min_samples_leaf=25, min_samples_split=16, n_estimators=34;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=16, max_features=28, max_leaf_nodes=28, min_samples_leaf=25, min_samples_split=16, n_estimators=34;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=16, max_features=28, max_leaf_nodes=28, min_samples_leaf=25, min_samples_split=16, n_estimators=34;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=19, max_features=27, max_leaf_nodes=39, min_samples_leaf=27, min_samples_split=9, n_estimators=160;, score=nan total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=19, max_features=27, max_leaf_nodes=39, min_samples_leaf=27, min_samples_split=9, n_estimators=160;, score=nan total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=19, max_features=27, max_leaf_nodes=39, min_samples_leaf=27, min_samples_split=9, n_estimators=160;, score=nan total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=19, max_features=27, max_leaf_nodes=39, min_samples_leaf=27, min_samples_split=9, n_estimators=160;, score=nan total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=19, max_features=27, max_leaf_nodes=39, min_samples_leaf=27, min_samples_split=9, n_estimators=160;, score=nan total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=16, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=8, n_estimators=70;, score=0.783 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=16, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=8, n_estimators=70;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=16, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=8, n_estimators=70;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=16, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=8, n_estimators=70;, score=0.783 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=16, max_features=8, max_leaf_nodes=28, min_samples_leaf=4, min_samples_split=8, n_estimators=70;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=47, min_samples_leaf=14, min_samples_split=7, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=47, min_samples_leaf=14, min_samples_split=7, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=47, min_samples_leaf=14, min_samples_split=7, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=47, min_samples_leaf=14, min_samples_split=7, n_estimators=130;, score=0.800 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=8, max_leaf_nodes=47, min_samples_leaf=14, min_samples_split=7, n_estimators=130;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=13, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=21, n_estimators=170;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=13, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=21, n_estimators=170;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=13, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=21, n_estimators=170;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=13, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=21, n_estimators=170;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=13, max_leaf_nodes=26, min_samples_leaf=10, min_samples_split=21, n_estimators=170;, score=0.814 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=16, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=5, n_estimators=171;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=16, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=5, n_estimators=171;, score=0.800 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=16, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=5, n_estimators=171;, score=0.800 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=16, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=5, n_estimators=171;, score=0.800 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=16, max_leaf_nodes=38, min_samples_leaf=12, min_samples_split=5, n_estimators=171;, score=0.814 total time=   0.2s\n",
      "{'n_estimators': 130, 'min_samples_split': 14, 'min_samples_leaf': 12, 'max_leaf_nodes': 45, 'max_features': 10, 'max_depth': 19, 'criterion': 'gini'}\n",
      "0.74373795761079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# RandomForestClassifier().get_params().keys()\n",
    "dt = RandomForestClassifier(random_state=30)\n",
    "params = {\n",
    "    'n_estimators': range(1,200,1),\n",
    "    'min_samples_leaf':np.arange(1, 30, 1),\n",
    "    'min_samples_split':np.arange(2, 2+20, 1),\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_features':np.arange(5,30,1),\n",
    "    'max_depth':np.arange(1, 20, 1),\n",
    "    'max_leaf_nodes':np.arange(25,50,1),\n",
    "}\n",
    "random_search = RandomizedSearchCV(dt, param_distributions=params, n_iter=200,\n",
    "                                   verbose=3, random_state=1001)\n",
    "random_search.fit(x_train, y_train)\n",
    "print(random_search.best_params_)\n",
    "print(random_search.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多模型比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型性能评估\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_matrics(y_test, y_pred, labels=['0', '1']):\n",
    "\tconf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # 根据混淆矩阵，获取对应的参数值\n",
    "\tTP = conf_matrix[1, 1]\n",
    "\tFN = conf_matrix[1, 0]\n",
    "\tFP = conf_matrix[0, 1]\n",
    "\tTN = conf_matrix[0, 0]\n",
    "\tshow_metrics = [[FN, FP], [TN, TP]]\n",
    "\tfig, ax = plt.subplots(figsize=(8, 8))\n",
    "\timg = ax.imshow(show_metrics, cmap=plt.get_cmap(\"Blues\"))\n",
    "\tn = len(labels)\n",
    "\tax.set_title('Confusion Matrix')\n",
    "\tax.set_xticks(range(n))\n",
    "\tax.set_xticklabels(labels)\n",
    "\tax.set_yticks(range(n))\n",
    "\tax.set_yticklabels(labels)\n",
    "\tax.set_xlabel('Predicted Label')\n",
    "\tax.set_ylabel('True Label')\n",
    "\tfor i in range(n):\n",
    "\t\tfor j in range(n):\n",
    "\t\t\ttext = f\"{show_metrics[i][j]}\"\n",
    "\t\t\tplt.text(j, i, text, horizontalalignment=\"center\",\n",
    "\t\t\t\t\t color=\"red\")\n",
    "\t\n",
    "\tplt.colorbar(img)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:01:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:01:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:01:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__gamma\", \"classifier__max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:01:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "模型XGBoost：Acc值为：0.75 \t AUC值为0.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型Decision Tree：Acc值为：0.74 \t AUC值为0.55\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型Random Forest：Acc值为：0.74 \t AUC值为0.62\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdQAAAHHCAYAAAAGZalZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAl70lEQVR4nO3de7wfVX3v/9c7CQSUO0HABAQB7UE5okXAaxGrAvUU7LGC9VRQe7AtWE/VVmz7qJeq1bZq60P0V6wXvFTAIj8BuQqipV64FZGLlBwukoBCQBC5hSSf88d3gpuw987OznznuzP79cxjHvs7a2bWrO9+hHz4rDVrVqoKSZK0fuaMugGSJPWBAVWSpBYYUCVJaoEBVZKkFhhQJUlqgQFVkqQWzBt1AyRJs8vcLZ5SteLB1uutB+88t6oOar3iKTKgSpI6VSseZP7TX9N6vQ9defyC1itdBwZUSVLHAunfiGP/vpEkSSNghipJ6laAZNStaJ0ZqiRJLTBDlSR1r4djqAZUSVL37PKVJEnjMUOVJHXMaTOSJGkCZqiSpO71cAzVgCpJ6lawy1eSJI3PDFWS1LH0ssvXDFWSpBaYoUqSutfDMVQDqiSpe3b5SpKk8ZihSpI65puSJEnSBMxQJUndcoFxSZI0EQOqZoUkmyY5I8m9Sb66HvW8Lsl5bbZtFJKcneTIUbdDs1jmtL+N2OhbII2R5PeSXJbkl0lub/7hf2ELVb8a2B7Ytqp+d7qVVNWXq+rlLbTnMZIckKSSnLZG+bOa8oumWM97knxpbedV1cFVdeI0myutpxhQpWFK8jbgH4EPMgh+OwOfBA5tofqnAP9VVStaqGtY7gSel2TbMWVHAv/V1g0y4H/30hD4H5ZmhCRbAu8Djqmqr1XV/VX1SFWdUVV/1pwzP8k/Jrmt2f4xyfzm2AFJliR5e5I7muz2Dc2x9wJ/DRzeZL5vWjOTS7JLkwnOa/aPSnJjkvuS3JTkdWPKLx5z3fOTXNp0JV+a5Pljjl2U5G+S/EdTz3lJFkzya1gO/P/AEc31c4HDgS+v8bv6pyS3JvlFksuTvKgpPwj4izHf84dj2vGBJP8BPAA8tSn7g+b4p5KcOqb+Dye5IOnhUyOaOeak/W3UX2nUDZAazwM2AU6b5Jy/BPYH9gaeBewL/NWY4zsAWwILgTcBxyfZuqrezSDrPbmqNquqz0zWkCRPBD4OHFxVmwPPB64c57xtgG80524LfBT4xhoZ5u8BbwCeBGwMvGOyewNfAF7ffH4FcDVw2xrnXMrgd7AN8K/AV5NsUlXnrPE9nzXmmt8HjgY2B25Zo763A3s1/7PwIga/uyOrqtbSVkljGFA1U2wLLFtLl+zrgPdV1R1VdSfwXgaBYrVHmuOPVNVZwC+Bp0+zPauAZybZtKpur6prxjnnt4AbquqLVbWiqr4C/Bj4H2PO+VxV/VdVPQicwiAQTqiqvgtsk+TpDALrF8Y550tVdVdzz48A81n79/x8VV3TXPPIGvU9wOD3+FHgS8BbqmrJWuqTpm/1eqiOoUpDcRewYHWX6wSezGOzq1uaskfrWCMgPwBstq4Nqar7GXS1/iFwe5JvJPm1KbRndZsWjtn/6TTa80XgWOAljJOxJ3lHkuuabuZ7GGTlk3UlA9w62cGq+gFwI4N/6k6ZQhul9ZO0v42YAVUzxfeAh4HDJjnnNgYPF622M4/vDp2q+4EnjNnfYezBqjq3ql4G7Mgg6/z0FNqzuk1Lp9mm1b4I/DFwVpM9Pqrpkv1z4DXA1lW1FXAvg0AIMFE37aTdt0mOYZDp3tbUL2kdGVA1I1TVvQweHDo+yWFJnpBkoyQHJ/m75rSvAH+VZLvm4Z6/ZtBFOR1XAi9OsnPzQNS7Vh9Isn2SQ5ux1IcZdB2vGqeOs4CnNVN95iU5HNgTOHOabQKgqm4CfoPBmPGaNgdWMHgieF6Svwa2GHP8Z8Au6/Ikb5KnAe8H/heDrt8/T7L39FovTYXTZqShasYD38bgQaM7GXRTHsvgyVcY/KN/GXAV8CPgiqZsOvc6Hzi5qetyHhsE5zTtuA24m0Fw+6Nx6rgLeCWDh3ruYpDZvbKqlk2nTWvUfXFVjZd9nwucw2AqzS3AQzy2O3f1SyvuSnLF2u7TdLF/CfhwVf2wqm5g8KTwF1c/QS1pauKDfJKkLs3ZYlHN3+8trdf70DePu7yq9mm94iny5fiSpO7NgC7atvXvG0mSNAJmqJKkbs2QaS5tM0OVJPVekk2SXJLkh0muaV5JSpLPN68XvbLZ9m7Kk+TjSRYnuSrJc9Z2DzNUSVL3uh9DfRg4sKp+mWQj4OIkZzfH/qyq/m2N8w8G9mi2/YBPNT8nNKMC6uZbbVPbPXmnUTdDWm9bb7rRqJsgrbdbbrmZZcuWDadvtuMu3+bd1L9sdjdqtsmmuRwKfKG57vtJtkqyY1XdPtEFMyqgbvfknfjgl88adTOk9XbYXgvXfpI0w71gv5HNQJmuBUkuG7N/QlWdsHqnWcHpcmB34Piq+kGSPwI+0Lwk5QLguKp6mMErRMfO8V7SlG0YAVWSNBtkWF2+yyabh1pVK4G9k2wFnJbkmQzekvZTBqtBnQC8k8FSkuvMh5IkSbNKVd0DfAs4qFlNqpqs9HMMloWEwTu5x45BLmIt7+k2oEqSutfxajPNO8C3aj5vCrwM+HGSHZuyMFic4+rmktOB1zdP++4P3DvZ+CnY5StJmh12BE5sxlHnAKdU1ZlJLkyyHYMVm65ksGwjDBa/OARYzGDpxTes7QYGVElSt1YvMN6hqroKePY45QdOcH4Bx6zLPQyokqSODe2hpJHq3zeSJGkEzFAlSd3zXb6SJGk8ZqiSpO71cAzVgCpJ6p5dvpIkaTxmqJKkbsVpM5IkaQJmqJKk7vVwDNWAKknqXHoYUO3ylSSpBWaokqROBTNUSZI0ATNUSVK30mw9Y4YqSVILzFAlSR1LL8dQDaiSpM71MaDa5StJUgvMUCVJnTNDlSRJ4zJDlSR1ro8ZqgFVktQt56FKkqSJmKFKkjqVns5DNUOVJKkFZqiSpM71MUM1oEqSOtfHgGqXryRJLTBDlSR1zgxVkiSNywxVktQtX+wgSZImYoYqSepcH8dQDaiSpE75piRJkjQhM1RJUufMUCVJ0rjMUCVJ3etfgmpAlSR1LHb5SpKkCZihSpI6Z4YqSZLGZYYqSepcHzNUA6okqVO+KUmSJE3IDFWS1L3+JahmqJKk/kuySZJLkvwwyTVJ3tuU75rkB0kWJzk5ycZN+fxmf3FzfJe13cOAKknqVvNih7a3tXgYOLCqngXsDRyUZH/gw8DHqmp34OfAm5rz3wT8vCn/WHPepAyokqTeq4FfNrsbNVsBBwL/1pSfCBzWfD602ac5/tKsJWobUCVJnRtBhkqSuUmuBO4Azgf+L3BPVa1oTlkCLGw+LwRuBWiO3wtsO1n9PpQkSerckKbNLEhy2Zj9E6rqhNU7VbUS2DvJVsBpwK+1eXMDqiSpL5ZV1T5rO6mq7knyLeB5wFZJ5jVZ6CJgaXPaUmAnYEmSecCWwF2T1WuXrySpexnCNtntku2azJQkmwIvA64DvgW8ujntSODrzefTm32a4xdWVU12DzNUSdJssCNwYpK5DJLJU6rqzCTXAicleT/wn8BnmvM/A3wxyWLgbuCItd3AgCpJ6lzXrx6sqquAZ49TfiOw7zjlDwG/uy73MKBKkjo11adyNzSOoUqS1AIzVElS58xQJUnSuMxQJUmd62OGakCVJHWvf/HULl9JktpghipJ6lwfu3zNUCVJaoEZqiSpWzFDlSRJEzBDlSR1KkAPE1QDqiSpa77LV5IkTcAMVZLUuR4mqGaokiS1wQxVktS5Po6hGlAlSd1KP7t8Dag9s+973s6T//2bPLTNAs756gUA7PXJv2fhRedSc+bw8DYL+P57P8pD2+3AwovOZa9P/j01Zw41dx5XvOM9LHv2viP+BtLk5px7DvPe9lZYuZKVb/wDVv75caNukgQ4hto7N/2P3+Xbn/jSY8que/0fcs4p3+Tck85j6YteyjNP+EcAfrbvCznn5PM596Tz+MG7/4F9/+bPRtBiaR2sXMm8PzmGR844m+VXXcvck75Crr121K3SOgowZ05a30bNgNozd/76/izfcqvHlK3YbPNHP8978EGq6WtZ8YQnPtrvMu/BB6k+rqekXskll1C77U499amw8casPPwI5pzx9VE3SwLs8p019vrEh9n1G//G8s224FsnnPJo+cILz+ZZn/gQ8+9exnf+6QsjbKG0drltKbVop0f3a+Ei5lzygxG2SNPVxzHUoWaoSQ5Kcn2SxUkc6BihHx37Tk4/+1JuOfhV7HHS5x4tX3rgwZz1tW9z8Uc+w16f+vsRtlDSbJKk9W3UhhZQk8wFjgcOBvYEXptkz2HdT1Nzy8GvYtGFZz+u/M5f35/Nlv6EjX9+9whaJU1NPXkhWXLro/tZuoRauHCELZJ+ZZgZ6r7A4qq6saqWAycBhw7xfprAZj+58dHPC799LvftsltTfhNUAbD1dT9izvKHWb7V1iNpozQV9dznksU3kJtuguXLmXvySax65W+PullaV820mba3URvmGOpC4NYx+0uA/YZ4PwHPe9cxPOny7zH/nrv57YP24eo/fDs7Xnwhm99yIyTcv+MiLvvLvwVg0YVnseuZp7Jq3jxWzt+E737oUzPjb6U0kXnzWPFPn2Cj33rFYNrMUW+knvGMUbdKAmbAQ0lJjgaOBliwg1036+t7f3v848puPOy1457746OO4cdHHTPsJkmtWnXwISw/+JBRN0PrYbB8W//+532YXb5LgZ3G7C9qyh6jqk6oqn2qap/Nt952iM2RJGl4hpmhXgrskWRXBoH0COD3hng/SdIGYWY8ldu2oQXUqlqR5FjgXGAu8NmqumZY95MkbTh6GE+HO4ZaVWcBZw3zHpIkzQQjfyhJkjT79LHL13f5SpLUAjNUSVK3ZsiLGNpmQJUkdcp5qJIkaUJmqJKkzvUwQTVDlSSpDWaokqTO9XEM1YAqSepcD+OpXb6SJLXBDFWS1K30s8vXDFWSpBaYoUqSOjV4scOoW9E+M1RJklpghipJ6pgLjEuS1IoexlO7fCVJs0OSnZJ8K8m1Sa5J8tam/D1Jlia5stkOGXPNu5IsTnJ9kldMVr8ZqiSpcyPq8l0BvL2qrkiyOXB5kvObYx+rqn8Ye3KSPYEjgGcATwa+meRpVbVyvMrNUCVJs0JV3V5VVzSf7wOuAxZOcsmhwElV9XBV3QQsBvad6GQDqiSpW80C421v69SEZBfg2cAPmqJjk1yV5LNJtm7KFgK3jrlsCZMEYAOqJKlTqxcYb3sDFiS5bMx29Lj3TzYDTgX+T1X9AvgUsBuwN3A78JHpfC/HUCVJfbGsqvaZ7IQkGzEIpl+uqq8BVNXPxhz/NHBms7sU2GnM5YuasnGZoUqSOjekDHVt9wzwGeC6qvromPIdx5z2KuDq5vPpwBFJ5ifZFdgDuGSi+s1QJUmzxQuA3wd+lOTKpuwvgNcm2Rso4GbgzQBVdU2SU4BrGTwhfMxET/iCAVWSNAKjmDVTVRczGMJd01mTXPMB4ANTqd+AKknqXB9fPegYqiRJLTBDlSR1axrzRjcEZqiSJLXADFWS1Km4fJskSe3oYTy1y1eSpDaYoUqSOjenhymqGaokSS0wQ5Ukda6HCaoZqiRJbTBDlSR1arAgeP9SVAOqJKlzc/oXT+3ylSSpDWaokqTO9bHL1wxVkqQWmKFKkjrXwwTVgCpJ6lYYvCC/b+zylSSpBWaokqTOOW1GkiSNywxVktStuMC4JEmt6GE8tctXkqQ2mKFKkjoVXGBckiRNwAxVktS5HiaoZqiSJLXBDFWS1DmnzUiStJ4Su3wlSdIEzFAlSZ1z2owkSRqXGaokqXP9y08NqJKkEZhVT/kmec5kF1bVFe03R5KkDdNkGepHJjlWwIEtt0WSNAsM3uU76la0b8KAWlUv6bIhkiRtyNY6hprkCcDbgJ2r6ugkewBPr6ozh946SVL/9HSB8alMm/kcsBx4frO/FHj/0FokSeq91W9LanMbtakE1N2q6u+ARwCq6gH6+cSzJEnTNpVpM8uTbMrgQSSS7AY8PNRWSZJ6rY9dvlMJqO8GzgF2SvJl4AXAUcNslCRJG5q1BtSqOj/JFcD+DLp631pVy4beMklSL826aTNr+A3ghQy6fTcCThtaiyRJ2gBNZdrMJ4Hdga80RW9O8ptVdcxQWyZJ6q3ZOoZ6IPDfqmr1Q0knAtcMtVWSpF7rXzid2rSZxcDOY/Z3asokSVJjwoCa5IwkpwObA9cluSjJt4DrmjJJktZZMlhgvO1t7ffNTkm+leTaJNckeWtTvk2S85Pc0PzcuilPko8nWZzkqrUtGjNZl+8/rMsvSJKkGW4F8PaquiLJ5sDlSc5nMBX0gqr6UJLjgOOAdwIHA3s0237Ap5qf45rs5fjfbu0rSJI0xiieSaqq24Hbm8/3JbkOWAgcChzQnHYicBGDgHoo8IXmGaLvJ9kqyY5NPY+z1jHUJPsnuTTJL5MsT7IyyS/W94tJkmavNC/Ib3MDFiS5bMx29CT33wV4NvADYPsxQfKnwPbN54XArWMuW9KUjWsqT/l+AjgC+CqwD/B64GlTuE6SpC4tq6p91nZSks2AU4H/U1W/GDuFp6oqSU3n5lN5ypeqWgzMraqVVfU54KDp3EySJBjdajNJNmIQTL9cVV9rin+WZMfm+I7AHU35UgYzW1Zb1JSNayoB9YEkGwNXJvm7JH86xeskSZoxMkhFPwNcV1UfHXPodODI5vORwNfHlL++edp3f+DeicZPYWpdvr/PIIAeC/wpg2j9O+v0LSRJaoSpTXMZghcwiGk/SnJlU/YXwIeAU5K8CbgFeE1z7CzgEAbvXngAeMNklU/l5fi3NB8fAt4LkORk4PB1+RaSJAEwogXBq+piJn5J00vHOb+AKb9md7pdt8+b5nWSJPXSVFebkSSpNbPq5fiTvGIpDJZwa93NN9/OG974t8OoWurUYZd+YtRNkNSxyTLUj0xy7MdtN0SSNHv0carIZK8efEmXDZEkaUPmGKokqVNhlo2hSpI0LHP6F0972Y0tSVLn1pqhNq9qeh3w1Kp6X5KdgR2q6pKht06S1EuzNUP9JIMXOby22b8POH5oLZIkaQM0lTHU/arqOUn+E6Cqft68LF+SpHU2WB2mfynqVALqI0nmAgWQZDtg1VBbJUnqtdna5ftx4DTgSUk+AFwMfHCorZIkaQMzldVmvpzkcgZv4g9wWFVdN/SWSZJ6q4c9vlN6yndnBuvAnTG2rKp+MsyGSZK0IZnKGOo3GIyfBtgE2BW4HnjGENslSeqpwKgWGB+qqXT57jV2v1mF5o+H1iJJUu/18a1C6/ydquoKYL8htEWSpA3WVMZQ3zZmdw7wHOC2obVIktR7PezxndIY6uZjPq9gMKZ66nCaI0nShmnSgNq80GHzqnpHR+2RJPVckl4+lDThGGqSeVW1EnhBh+2RJGmDNFmGegmD8dIrk5wOfBW4f/XBqvrakNsmSeqpHiaoUxpD3QS4CziQX81HLcCAKkmalj6+y3eygPqk5gnfq/lVIF2thtoqSZI2MJMF1LnAZjw2kK5mQJUkTctsfFPS7VX1vs5aIknSBmyygNq//32QJM0IPUxQJw2oL+2sFZKk2SP9fChpwnmoVXV3lw2RJGlDNpVpM5IktSo9HFXs4wo6kiR1zgxVktSpwbSZUbeifQZUSVLn+hhQ7fKVJKkFZqiSpM6lhxNRzVAlSWqBGaokqVN9fSjJDFWSpBaYoUqSupXZ9y5fSZKGoo/Lt9nlK0lSC8xQJUmd8qEkSZI0ITNUSVLnejiEakCVJHUtzHH5NkmSNB4DqiSpU2HQ5dv2ttb7Jp9NckeSq8eUvSfJ0iRXNtshY469K8niJNcnecXa6jegSpJmi88DB41T/rGq2rvZzgJIsidwBPCM5ppPJpk7WeUGVElStzKYNtP2tjZV9R3g7im28lDgpKp6uKpuAhYD+052gQFVktS5OUnr23o4NslVTZfw1k3ZQuDWMecsacom/k7r0wJJkmaQBUkuG7MdPYVrPgXsBuwN3A58ZLo3d9qMJKlTqx9KGoJlVbXPulxQVT9b/TnJp4Ezm92lwE5jTl3UlE3IDFWSNGsl2XHM7quA1U8Anw4ckWR+kl2BPYBLJqvLDFWS1LlRrDaT5CvAAQy6hpcA7wYOSLI3UMDNwJsBquqaJKcA1wIrgGOqauVk9RtQJUmzQlW9dpziz0xy/geAD0y1fgOqJKlzvstXkqT1FPr5AE8fv5MkSZ0zQ5UkdSuQHvb5mqFKktQCM1RJUuf6l58aUCVJHQujmYc6bHb5SpLUAjNUSVLn+pefmqFKktQKM1RJUud6OIRqQJUkdS3OQ5UkSeMzQ5Ukdcp3+UqSpAmZoUqSOucYqiRJGpcZqiSpc/3LTw2okqSuuXybJEmaiBmqJKlTTpuRJEkTMkOVJHWuj2OoBlRJUuf6F07t8pUkqRVmqJKkzvWwx9cMVZKkNpihSpI6NZg2078U1YDaI/NXreCbi09j41UrmccqTttyN96/434ccN+tfPC27zKnivvnbsz/3vlAbpy/FQD/8+c38Jc/vZQK/GiTBRy1y8tH+yWktZhz7jnMe9tbYeVKVr7xD1j558eNukmahj52+RpQe+ThzOWg3Q7l/rkbM69WcuENX+O8LZ7Cx5d8m9/d9RCu32Qbjl72I4776eUc/ZSXstvD9/COO67gwD1+h3vmbcJ2jzww6q8gTW7lSub9yTE8cvb51KJFbLz/c1n1yt+m9txz1C2THEPtlYT7524MwEa1inm1igKKsMXK5QBssXI5t2/0BADeeNe1/POCvbhn3iYA3NmUSzNVLrmE2m136qlPhY03ZuXhRzDnjK+PullaZxnKn1EzQ+2ZObWK715/Crstv5d/XrAXlz5xB/54p5dw2o1n8tCcefxizsb8xtNeDcAeD90DwIU3nMrcKt6/w3M5f4unjLD10uRy21Jq0U6P7tfCRcy55AcjbJH0K0PLUJN8NskdSa4e1j30eKsyh/1/7Qh23/Mo9nngDvZ88C7ecucPedVTX8nuzziKL277a3x46cUAzGUVuz98Dy/f/TBe/5SX88lbL2LLFQ+P9gtImhWS9rdRG2aX7+eBg4ZYvyZx77z5fHuzhbzivlvY68FlXPrEHQD4t632YP/7fwrA0o0248wtdmVF5nLL/C24Yf6W7L78nhG2WppcPXkhWXLro/tZuoRauHCELdJ0rH7Kt+1t1IYWUKvqO8Ddw6pfj7dgxYOPZpibrFrBS++7lR/P34YtVi5n96Z798D7buX6TbYG4Iwtd+XFv1wKwLYrHmSPh+/lpo23HEnbpamo5z6XLL6B3HQTLF/O3JNPYtUrf3vUzZIAx1B7ZYdH7ufTP7mAuVXMoTh1q905e8tdOGanl/CVm89mFeGeufN5884HAnD+5jvzm/fdyhXX/SsrE/7iyc/n7uYBJWlGmjePFf/0CTb6rVcMps0c9UbqGc8Ydau0rmZIF23bRh5QkxwNHA3ARpuNtjEbuKs3XcDznn7448pP3+qpnL7VUx9/QcI7F76Qd9pjpg3IqoMPYfnBh4y6GdLjjDygVtUJwAkAc57wpBpxcyRJHehjhuo8VEmSWjDMaTNfAb4HPD3JkiRvGta9JEkbFl/ssA6q6rXDqluStOEKMGf08a91dvlKktSCkT+UJEmafWZCF23bzFAlSWqBGaokqXN9nDZjQJUkdc4uX0mSNC4DqiSpU6unzbS9rfW+4ywrmmSbJOcnuaH5uXVTniQfT7I4yVVJnrO2+g2okqTZ4vM8flnR44ALqmoP4IJmH+BgYI9mOxr41NoqN6BKkjo2jPckrT1FnWBZ0UOBE5vPJwKHjSn/Qg18H9gqyY6T1e9DSZKkbs2s5du2r6rbm88/BbZvPi8Ebh1z3pKm7HYmYECVJPXFgiSXjdk/oVnRbEqqqpJMe9UzA6okqXNDSlCXVdU+63jNz5LsWFW3N126dzTlS4Gdxpy3qCmbkGOokqTZ7HTgyObzkcDXx5S/vnnad3/g3jFdw+MyQ5UkdWowbab7QdRmWdEDGHQNLwHeDXwIOKVZYvQW4DXN6WcBhwCLgQeAN6ytfgOqJGlWmGRZ0ZeOc24Bx6xL/QZUSVLnZs5Dvu0xoEqSutfDiOpDSZIktcAMVZLUOVebkSRJ4zJDlSR1bga9erA1BlRJUud6GE/t8pUkqQ1mqJKk7vUwRTVDlSSpBWaokqROhX5OmzGgSpK6NbMWGG+NXb6SJLXADFWS1LkeJqhmqJIktcEMVZLUvR6mqGaokiS1wAxVktSxOG1GkqQ2OG1GkiSNywxVktSp0MtnksxQJUlqgxmqJKl7PUxRDaiSpM718Slfu3wlSWqBGaokqXNOm5EkSeMyQ5Ukda6HCaoBVZLUsZ5ORLXLV5KkFpihSpI657QZSZI0LjNUSVKngtNmJEnSBMxQJUmd62GCakCVJI1ADyOqXb6SJLXADFWS1DmnzUiSpHGZoUqSOtfHaTMGVElS53oYT+3ylSSpDWaokqTu9TBFNUOVJKkFZqiSpE4NlkPtX4pqQJUkdSv9fMrXLl9JklpghipJ6lwPE1QDqiRp9khyM3AfsBJYUVX7JNkGOBnYBbgZeE1V/Xxd67bLV5LUvQxhm7qXVNXeVbVPs38ccEFV7QFc0OyvMwOqJGm2OxQ4sfl8InDYdCoxoEqSOpah/AEWJLlszHb0ODcv4Lwkl485vn1V3d58/imw/XS+lWOokqTODWnazLIx3bgTeWFVLU3yJOD8JD8ee7CqKklN5+ZmqJKkWaOqljY/7wBOA/YFfpZkR4Dm5x3TqduAKknq1DCeR5pKwpvkiUk2X/0ZeDlwNXA6cGRz2pHA16fzvezylSTNFtsDp2XQ3zwP+NeqOifJpcApSd4E3AK8ZjqVG1AlSd0bwZsdqupG4FnjlN8FvHR96zegSpI618eX4zuGKklSC8xQJUmdc7UZSZI0LjNUSVLnepigGlAlSR1zgXFJkjQRM1RJ0gj0L0U1Q5UkqQVmqJKkTgXHUCVJ0gTMUCVJnethgjqzAmo9eOeyh648/pZRt6PnFgDLRt2Ivtt0o+NH3YS+8+9xN54yrIr72OU7swJq1XajbkPfJblsCivaSzOaf481E82ogCpJmh1cbUaSJI3LDHX2OWHUDZBa4N/jDV3/ElQD6mxTVf5DpA2ef483fD2Mp3b5SpLUBgPqLJHkoCTXJ1mc5LhRt0eajiSfTXJHkqtH3RZNXzKcbdQMqLNAkrnA8cDBwJ7Aa5PsOdpWSdPyeeCgUTdCGo8BdXbYF1hcVTdW1XLgJODQEbdJWmdV9R3g7lG3Q+svQ/gzagbU2WEhcOuY/SVNmSSNRoawjZgBVZKkFjhtZnZYCuw0Zn9RUyZJIzEDEsrWmaHODpcCeyTZNcnGwBHA6SNukyT1igF1FqiqFcCxwLnAdcApVXXNaFslrbskXwG+Bzw9yZIkbxp1mzQ9fZw2Y5fvLFFVZwFnjbod0vqoqteOug3SRAyokqSOzYxpLm0zoEqSOhVmRhdt2xxDlSSpBQZUSZJaYECVJKkFBlRt0JKsTHJlkquTfDXJE9ajrs8neXXz+V8mW0AgyQFJnj+Ne9ycZMFUyyeo46gkn2jjvtKo9HHajAFVG7oHq2rvqnomsBz4w7EHk0zrwbuq+oOqunaSUw4A1jmgShrw5fjSzPbvwO5N9vjvSU4Hrk0yN8nfJ7k0yVVJ3gyQgU8068R+E3jS6oqSXJRkn+bzQUmuSPLDJBck2YVB4P7TJjt+UZLtkpza3OPSJC9ort02yXlJrknyL6zDG9eS7Jvke0n+M8l3kzx9zOGdmjbekOTdY675X0kuadr1z83SfZI64LQZ9UKTiR4MnNMUPQd4ZlXdlORo4N6qem6S+cB/JDkPeDbwdAZrxG4PXAt8do16twM+Dby4qWubqro7yf8H/LKq/qE571+Bj1XVxUl2ZvBWqv8GvBu4uKrel+S3gHV5s8+PgRdV1Yokvwl8EPifzbF9gWcCDwCXJvkGcD9wOPCCqnokySeB1wFfWId7SsM3Q7po22ZA1YZu0yRXNp//HfgMg67YS6rqpqb85cB/Xz0+CmwJ7AG8GPhKVa0Ebkty4Tj17w98Z3VdVTXRWpy/CeyZX/0rsUWSzZp7/E5z7TeS/HwdvtuWwIlJ9gAK2GjMsfOr6i6AJF8DXgisAH6dQYAF2BS4Yx3uJ2k9GFC1oXuwqvYeW9AEk/vHFgFvqapz1zjvkBbbMQfYv6oeGqct0/U3wLeq6lVNN/NFY47VGucWg+95YlW9a31uKg3bDFm+tHWOoWo2OBf4oyQbASR5WpInAt8BDm/GWHcEXjLOtd8HXpxk1+babZry+4DNx5x3HvCW1TtJ9m4+fgf4vabsYGDrdWj3lvxqmb2j1jj2siTbJNkUOAz4D+AC4NVJnrS6rUmesg73k7rjAuPSBulfGIyPXpHkauCfGfTOnAbc0Bz7AoNVTB6jqu4Ejga+luSHwMnNoTOAV61+KAn4E2Cf5qGna/nV08bvZRCQr2HQ9fuTSdp5VbOCypIkHwX+DvjbJP/J43uTLgFOBa4CTq2qy5qnkv8KOC/JVcD5wI5T/B1JWk+pWrPnSJKk4XnOr+9T3/nupa3Xu/kmcy6vqn1ar3iKzFAlSWqBDyVJkjrXx2kzZqiSJLXADFWS1LkeJqgGVEnSCPQwotrlK0maFZr3cl+fZHGS49qu3wxVktS5rleHaRaKOB54GbCEwSs6T1/LqlLrxAxVkjQb7Assrqobq2o5cBJwaJs3MEOVJHUqjGTazELg1jH7S4D92ryBAVWS1Kkrrrj83E03yoIhVL1JksvG7J9QVScM4T7jMqBKkjpVVQeN4LZLgZ3G7C/iV4tPtMIxVEnSbHApsEeSXZNsDBwBnN7mDcxQJUm9V1UrkhzLYDnHucBnq+qaNu/hajOSJLXALl9JklpgQJUkqQUGVEmSWmBAlSSpBQZUSZJaYECVJKkFBlRJklpgQJUkqQX/Dyg/nn9+qrt0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型AdaBoost：Acc值为：0.71 \t AUC值为0.51\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型LogisticRegression：Acc值为：0.74 \t AUC值为0.58\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型Nearest Neighbors：Acc值为：0.68 \t AUC值为0.52\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型Linear SVM：Acc值为：0.74 \t AUC值为0.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型RBF SVM：Acc值为：0.74 \t AUC值为0.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型Gaussian Process：Acc值为：0.70 \t AUC值为0.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型MLP：Acc值为：0.58 \t AUC值为0.37\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pylab import mpl\n",
    "import seaborn as sns\n",
    "plt.figure(facecolor='snow')\n",
    "font_size = 12\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [ \"XGBoost\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"LogisticRegression\",\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\", \"MLP\",\n",
    "         \"Naive Bayes\", \"QuadraticDiscriminantAnalysis\"]\n",
    "#n_estimators=100, random_state=0, algorithm=\"SAMME\",learning_rate=0.1\n",
    "classifiers = [\n",
    "    XGBClassifier(subsample=0.2, min_child_weight=5, classifier__max_depth=7, classifier__gamma=0.73,\n",
    "                        classifier__colsample_bytree=1,eta= 0.31,base_score=0.66515),\n",
    "    DecisionTreeClassifier(splitter='random', min_samples_leaf=16, max_depth=16, criterion='entropy'),\n",
    "    RandomForestClassifier(n_estimators= 130, min_samples_split= 14, min_samples_leaf= 12, max_leaf_nodes= 45, max_features=10, max_depth=19, criterion='gini'),\n",
    "    AdaBoostClassifier(DecisionTreeClassifier(splitter='random', min_samples_leaf=16, max_depth=16, criterion='entropy')),\n",
    "    LogisticRegression(random_state=1),\n",
    "    KNeighborsClassifier(1),\n",
    "    SVC(kernel=\"linear\", C=0.025,probability=True),\n",
    "    SVC(gamma=0.73, C=1, probability=True),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    MLPClassifier(alpha=0.7, max_iter=1000)]\n",
    "    # GaussianNB(),\n",
    "    # QuadraticDiscriminantAnalysis()]\n",
    "plt.figure(figsize=(6,6))\n",
    "i = 0\n",
    "colors = ['darkorange','deepskyblue','lightgreen','lightcoral','forestgreen','pink','coral','lightblue', 'lightpink', 'lightyellow']\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_pred_proba = clf.predict_proba(x_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=name, #+\"ROC curve (area = %0.2f)\" % auc_score\n",
    "    )\n",
    "    i += 1\n",
    "    plt.plot([0, 1], [0, 1], color=\"black\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic example\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig('auc_roc.pdf')\n",
    "    # plt.show()\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    auc_score = roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1])\n",
    "    print(f\"模型%s：Acc值为：%.2f \\t AUC值为%.2f\"%(name,clf.score(x_test, y_test), auc_score))\n",
    "    show_matrics(y_test.squeeze(), y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99f90fdae1a5a7fec23a8436b9db5ac2c3500aa685f5f081db9d052354c33f95"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit ('zhappy-dien': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
